<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 Linear Regression | Applied Linear Models</title>
  <meta name="description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 Linear Regression | Applied Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Linear Regression | Applied Linear Models" />
  
  <meta name="twitter:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  

<meta name="author" content="Nick Syring" />


<meta name="date" content="2023-02-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="k-factorial-experiments.html"/>
<link rel="next" href="analysis-of-covariance.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html"><i class="fa fa-check"></i><b>2</b> Data Analysis and Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#data-experiments-and-studies"><i class="fa fa-check"></i><b>2.1</b> Data, Experiments, and Studies</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#james-linds-scurvy-trial"><i class="fa fa-check"></i><b>2.1.1</b> James Lind’s Scurvy Trial</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#framingham-heart-study"><i class="fa fa-check"></i><b>2.1.2</b> Framingham Heart Study</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#harris-bank-sex-pay-study"><i class="fa fa-check"></i><b>2.1.3</b> Harris Bank Sex Pay Study</a></li>
<li class="chapter" data-level="2.1.4" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#large-aggregated-data-sets"><i class="fa fa-check"></i><b>2.1.4</b> Large, Aggregated Data Sets</a></li>
<li class="chapter" data-level="2.1.5" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#study-concepts"><i class="fa fa-check"></i><b>2.1.5</b> Study Concepts</a></li>
<li class="chapter" data-level="2.1.6" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#randomization-control-and-causation"><i class="fa fa-check"></i><b>2.1.6</b> Randomization, control, and causation</a></li>
<li class="chapter" data-level="2.1.7" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#populations-and-scope-of-inference"><i class="fa fa-check"></i><b>2.1.7</b> Populations and scope of inference</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#data-summaries"><i class="fa fa-check"></i><b>2.2</b> Data Summaries</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#numerical-summaries"><i class="fa fa-check"></i><b>2.2.1</b> Numerical Summaries</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#visual-summaries"><i class="fa fa-check"></i><b>2.2.2</b> Visual Summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#statistical-inference"><i class="fa fa-check"></i><b>2.3</b> Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html"><i class="fa fa-check"></i><b>3</b> Introduction to Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#defining-the-linear-model"><i class="fa fa-check"></i><b>3.1</b> Defining the linear model</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#gauss-markov-model-for-one-sample"><i class="fa fa-check"></i><b>3.2</b> Gauss-Markov model for one sample</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#gauss-markov-model-for-comparing-two-samples"><i class="fa fa-check"></i><b>3.3</b> Gauss-Markov model for comparing two samples</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#pairwise-testing-and-bonferroni-correction"><i class="fa fa-check"></i><b>3.4</b> Pairwise testing and Bonferroni correction</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#example-co2-uptake-in-echinochloa-crus-galli"><i class="fa fa-check"></i><b>3.4.1</b> Example: Co2 uptake in Echinochloa crus-galli</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#linear-models-for-more-than-two-treatments-populations"><i class="fa fa-check"></i><b>3.5</b> Linear Models for more than two treatments (populations)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html"><i class="fa fa-check"></i><b>4</b> Randomization and Permutation Testing</a>
<ul>
<li class="chapter" data-level="4.1" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#setup"><i class="fa fa-check"></i><b>4.1</b> Setup</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#example-1-the-harris-bank-sex-pay-study"><i class="fa fa-check"></i><b>4.1.1</b> Example 1: The Harris Bank Sex Pay Study</a></li>
<li class="chapter" data-level="4.1.2" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#difference-in-mean-salaries-between-genders"><i class="fa fa-check"></i><b>4.1.2</b> Difference in mean salaries between genders</a></li>
<li class="chapter" data-level="4.1.3" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#randomization-test-for-treatment-significance"><i class="fa fa-check"></i><b>4.1.3</b> Randomization test for treatment significance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html"><i class="fa fa-check"></i><b>5</b> Alternatives for Non-Normal Responses</a>
<ul>
<li class="chapter" data-level="5.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#example-cloud-seeding-for-rainfall"><i class="fa fa-check"></i><b>5.1.1</b> Example: Cloud Seeding for Rainfall</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#rank-sum-test"><i class="fa fa-check"></i><b>5.2</b> Rank-Sum Test</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#example-ratio-measurements"><i class="fa fa-check"></i><b>5.2.1</b> Example: ratio measurements</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#signed-rank-test"><i class="fa fa-check"></i><b>5.3</b> Signed-rank test</a></li>
<li class="chapter" data-level="5.4" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrap"><i class="fa fa-check"></i><b>5.4</b> Bootstrap</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#intro-to-the-bootstrap"><i class="fa fa-check"></i><b>5.4.1</b> Intro to the bootstrap</a></li>
<li class="chapter" data-level="5.4.2" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.4.2</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="5.4.3" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrapping-linear-models"><i class="fa fa-check"></i><b>5.4.3</b> Bootstrapping linear models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>6</b> One Way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1" data-path="one-way-anova.html"><a href="one-way-anova.html#the-gauss-markov-model-for-comparing-i-populations"><i class="fa fa-check"></i><b>6.1</b> The Gauss-Markov Model for comparing I populations</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="one-way-anova.html"><a href="one-way-anova.html#example-donuts-effects-model"><i class="fa fa-check"></i><b>6.1.1</b> Example: Donuts effects model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="one-way-anova.html"><a href="one-way-anova.html#testing-equality-of-means"><i class="fa fa-check"></i><b>6.2</b> Testing equality of means</a></li>
<li class="chapter" data-level="6.3" data-path="one-way-anova.html"><a href="one-way-anova.html#follow-up-testing"><i class="fa fa-check"></i><b>6.3</b> Follow-up testing</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="one-way-anova.html"><a href="one-way-anova.html#donuts-example"><i class="fa fa-check"></i><b>6.3.1</b> Donuts example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-way-anova.html"><a href="one-way-anova.html#inference-for-particular-contrasts"><i class="fa fa-check"></i><b>6.4</b> Inference for particular contrasts</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#donuts-example-1"><i class="fa fa-check"></i><b>6.4.1</b> Donuts Example</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="one-way-anova.html"><a href="one-way-anova.html#checking-assumptions-in-one-way-anova"><i class="fa fa-check"></i><b>6.5</b> Checking Assumptions in one-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="one-way-anova.html"><a href="one-way-anova.html#example-cehcking-assumptions-for-donuts-experiment"><i class="fa fa-check"></i><b>6.5.1</b> Example: Cehcking assumptions for donuts experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html"><i class="fa fa-check"></i><b>7</b> Randomized Complete Block Design</a>
<ul>
<li class="chapter" data-level="7.1" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#paired-experiments-as-blocking"><i class="fa fa-check"></i><b>7.1</b> Paired experiments as blocking</a></li>
<li class="chapter" data-level="7.2" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>7.2</b> Randomized Complete Block Designs</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#notation"><i class="fa fa-check"></i><b>7.2.1</b> Notation</a></li>
<li class="chapter" data-level="7.2.2" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-penicillin-manufacturing"><i class="fa fa-check"></i><b>7.2.2</b> Example: Penicillin Manufacturing</a></li>
<li class="chapter" data-level="7.2.3" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#sums-of-squares-and-f-tests-in-rcbd"><i class="fa fa-check"></i><b>7.2.3</b> Sums of squares and F tests in RCBD</a></li>
<li class="chapter" data-level="7.2.4" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-significance-of-process-in-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.4</b> Example: Significance of Process in Penicillin experiment</a></li>
<li class="chapter" data-level="7.2.5" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#follow-up-comparisons-and-contrasts"><i class="fa fa-check"></i><b>7.2.5</b> Follow-up comparisons and contrasts</a></li>
<li class="chapter" data-level="7.2.6" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-tukey-contrasts-and-scheffe-for-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.6</b> Example: Tukey, contrasts, and Scheffe for penicillin experiment</a></li>
<li class="chapter" data-level="7.2.7" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#relative-efficiency-of-blocking-vs.-crd"><i class="fa fa-check"></i><b>7.2.7</b> Relative efficiency of Blocking (vs. CRD)</a></li>
<li class="chapter" data-level="7.2.8" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-re-of-blocking-in-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.8</b> Example: RE of blocking in penicillin experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html"><i class="fa fa-check"></i><b>8</b> Latin Squares for two blocking variables</a>
<ul>
<li class="chapter" data-level="8.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#designs-for-multiple-blocks"><i class="fa fa-check"></i><b>8.1</b> Designs for multiple blocks</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-mpg-experiment"><i class="fa fa-check"></i><b>8.1.1</b> Example: MPG experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#model-notation"><i class="fa fa-check"></i><b>8.2</b> Model notation</a></li>
<li class="chapter" data-level="8.3" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#sums-of-squares-and-test-for-treatment-effects"><i class="fa fa-check"></i><b>8.3</b> Sums of squares and test for treatment effects</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-mpg-experiment-1"><i class="fa fa-check"></i><b>8.3.1</b> Example: MPG experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#relative-efficiency-of-blocking"><i class="fa fa-check"></i><b>8.4</b> Relative efficiency of blocking</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-re-ignoring-driver-blocks"><i class="fa fa-check"></i><b>8.4.1</b> Example: RE ignoring driver blocks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="two-way-anova.html"><a href="two-way-anova.html"><i class="fa fa-check"></i><b>9</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="9.1" data-path="two-way-anova.html"><a href="two-way-anova.html#the-model"><i class="fa fa-check"></i><b>9.1</b> The Model</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="two-way-anova.html"><a href="two-way-anova.html#example-protein-chemistry"><i class="fa fa-check"></i><b>9.1.1</b> Example: protein chemistry</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="two-way-anova.html"><a href="two-way-anova.html#tests-for-interaction-and-main-effects"><i class="fa fa-check"></i><b>9.2</b> Tests for interaction and main effects</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="two-way-anova.html"><a href="two-way-anova.html#example-protein-and-dietary-metals"><i class="fa fa-check"></i><b>9.2.1</b> Example: Protein and dietary metals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html"><i class="fa fa-check"></i><b>10</b> Unbalanced data in Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#raw-and-least-squares-means"><i class="fa fa-check"></i><b>10.1</b> Raw and Least-Squares Means</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data"><i class="fa fa-check"></i><b>10.1.1</b> Example: Turbine data</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#partial-f-tests-and-type-iii-ss"><i class="fa fa-check"></i><b>10.2</b> Partial F tests and Type III SS</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data-tests"><i class="fa fa-check"></i><b>10.2.1</b> Example: Turbine Data tests</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#follow-up-tests"><i class="fa fa-check"></i><b>10.3</b> Follow-up tests</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data-tukey-corrected-pairwise-comparisons"><i class="fa fa-check"></i><b>10.3.1</b> Example: Turbine data Tukey-corrected pairwise comparisons</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#unbalanced-data-and-simpsons-paradox"><i class="fa fa-check"></i><b>10.4</b> Unbalanced data and Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html"><i class="fa fa-check"></i><b>11</b> Missing Cells in Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#anova-with-missing-at-random-cell"><i class="fa fa-check"></i><b>11.1</b> ANOVA with missing “at random” cell</a></li>
<li class="chapter" data-level="11.2" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#do-any-of-the-treatments-matter"><i class="fa fa-check"></i><b>11.2</b> Do any of the treatments matter?</a></li>
<li class="chapter" data-level="11.3" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#fit-a-linear-model-with-additional-constraints"><i class="fa fa-check"></i><b>11.3</b> Fit a linear model with additional constraints</a></li>
<li class="chapter" data-level="11.4" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#general-linear-test-for-interaction"><i class="fa fa-check"></i><b>11.4</b> General Linear Test for interaction</a></li>
<li class="chapter" data-level="11.5" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#partial-analysis-for-interaction"><i class="fa fa-check"></i><b>11.5</b> Partial analysis for interaction</a></li>
<li class="chapter" data-level="11.6" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#tests-for-main-effects-in-the-partial-table"><i class="fa fa-check"></i><b>11.6</b> Tests for main effects in the partial table</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html"><i class="fa fa-check"></i><b>12</b> Strategy for Analysis in Two-Factor models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#unbalanced-two-factor-experiment-chick-weight"><i class="fa fa-check"></i><b>12.1</b> Unbalanced Two-Factor Experiment: Chick Weight</a></li>
<li class="chapter" data-level="12.2" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#example-observational-study-on-growth-hormone-deficient-children-with-empty-cell"><i class="fa fa-check"></i><b>12.2</b> Example: observational study on growth-hormone deficient children with empty cell</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#linear-model-analysis-with-extra-constraint"><i class="fa fa-check"></i><b>12.2.1</b> Linear Model analysis with extra constraint</a></li>
<li class="chapter" data-level="12.2.2" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#partial-table-analysis"><i class="fa fa-check"></i><b>12.2.2</b> Partial Table analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html"><i class="fa fa-check"></i><b>13</b> <span class="math inline">\(2^k\)</span> Factorial Experiments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#the-model-1"><i class="fa fa-check"></i><b>13.1</b> The Model</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#example-stress-test-study"><i class="fa fa-check"></i><b>13.1.1</b> Example: Stress Test Study</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#unreplicated-factorial-experiments"><i class="fa fa-check"></i><b>13.2</b> Unreplicated Factorial Experiments</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#example-granola-bar-experiment"><i class="fa fa-check"></i><b>13.2.1</b> Example: Granola bar experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>14</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="14.1" data-path="linear-regression.html"><a href="linear-regression.html#diamonds-dataset"><i class="fa fa-check"></i><b>14.1</b> Diamonds dataset</a></li>
<li class="chapter" data-level="14.2" data-path="linear-regression.html"><a href="linear-regression.html#checking-linearity"><i class="fa fa-check"></i><b>14.2</b> Checking Linearity</a></li>
<li class="chapter" data-level="14.3" data-path="linear-regression.html"><a href="linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>14.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="14.4" data-path="linear-regression.html"><a href="linear-regression.html#model-with-carat-interactions"><i class="fa fa-check"></i><b>14.4</b> Model with carat interactions</a></li>
<li class="chapter" data-level="14.5" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-the-model"><i class="fa fa-check"></i><b>14.5</b> Interpreting the model</a></li>
<li class="chapter" data-level="14.6" data-path="linear-regression.html"><a href="linear-regression.html#checking-constant-variance"><i class="fa fa-check"></i><b>14.6</b> Checking Constant Variance</a></li>
<li class="chapter" data-level="14.7" data-path="linear-regression.html"><a href="linear-regression.html#normality"><i class="fa fa-check"></i><b>14.7</b> Normality</a></li>
<li class="chapter" data-level="14.8" data-path="linear-regression.html"><a href="linear-regression.html#addressing-non-constant-variance-using-wls"><i class="fa fa-check"></i><b>14.8</b> Addressing non-constant variance using WLS</a></li>
<li class="chapter" data-level="14.9" data-path="linear-regression.html"><a href="linear-regression.html#inferences-using-wls"><i class="fa fa-check"></i><b>14.9</b> Inferences using WLS</a></li>
<li class="chapter" data-level="14.10" data-path="linear-regression.html"><a href="linear-regression.html#using-built-in-functions-in-r-for-inference"><i class="fa fa-check"></i><b>14.10</b> Using built-in functions in R for inference</a></li>
<li class="chapter" data-level="14.11" data-path="linear-regression.html"><a href="linear-regression.html#leverage-outliers-and-influence"><i class="fa fa-check"></i><b>14.11</b> Leverage, outliers, and influence</a>
<ul>
<li class="chapter" data-level="14.11.1" data-path="linear-regression.html"><a href="linear-regression.html#illustrations"><i class="fa fa-check"></i><b>14.11.1</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="linear-regression.html"><a href="linear-regression.html#numerical-summaries-of-outliers-leverage-and-influence"><i class="fa fa-check"></i><b>14.12</b> Numerical summaries of outliers, leverage, and influence</a></li>
<li class="chapter" data-level="14.13" data-path="linear-regression.html"><a href="linear-regression.html#leverage-diamonds-model"><i class="fa fa-check"></i><b>14.13</b> Leverage, Diamonds model</a>
<ul>
<li class="chapter" data-level="14.13.1" data-path="linear-regression.html"><a href="linear-regression.html#outliers-diamonds-model"><i class="fa fa-check"></i><b>14.13.1</b> Outliers, diamonds model</a></li>
<li class="chapter" data-level="14.13.2" data-path="linear-regression.html"><a href="linear-regression.html#plotting-outliers-with-leverage"><i class="fa fa-check"></i><b>14.13.2</b> plotting outliers with leverage</a></li>
<li class="chapter" data-level="14.13.3" data-path="linear-regression.html"><a href="linear-regression.html#cooks-distance-df-betas-df-fits"><i class="fa fa-check"></i><b>14.13.3</b> Cook’s distance, DF betas, DF fits</a></li>
<li class="chapter" data-level="14.13.4" data-path="linear-regression.html"><a href="linear-regression.html#model-fit-without-high-leverage-outliers"><i class="fa fa-check"></i><b>14.13.4</b> Model fit without high leverage outliers</a></li>
</ul></li>
<li class="chapter" data-level="14.14" data-path="linear-regression.html"><a href="linear-regression.html#dealing-with-highly-influential-data-points"><i class="fa fa-check"></i><b>14.14</b> Dealing with highly influential data points</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>15</b> Analysis of Covariance</a>
<ul>
<li class="chapter" data-level="15.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#choice-of-concommitant-variable"><i class="fa fa-check"></i><b>15.1</b> Choice of “concommitant” variable</a></li>
<li class="chapter" data-level="15.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#interaction-and-regression-slopes"><i class="fa fa-check"></i><b>15.2</b> Interaction and regression slopes</a></li>
<li class="chapter" data-level="15.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#inference-questions"><i class="fa fa-check"></i><b>15.3</b> Inference questions</a></li>
<li class="chapter" data-level="15.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#example"><i class="fa fa-check"></i><b>15.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html"><i class="fa fa-check"></i><b>16</b> Gauss-Markov Theory</a>
<ul>
<li class="chapter" data-level="16.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#estimability"><i class="fa fa-check"></i><b>16.1</b> Estimability</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-one-way-anova"><i class="fa fa-check"></i><b>16.1.1</b> Characterizing estimable functions in one-way ANOVA</a></li>
<li class="chapter" data-level="16.1.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-using-the-null-space-of-x"><i class="fa fa-check"></i><b>16.1.2</b> Characterizing estimable functions in RCBD using the null space of <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="16.1.3" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-by-solving-an-inhomogeneous-equation"><i class="fa fa-check"></i><b>16.1.3</b> Characterizing estimable functions in RCBD by solving an inhomogeneous equation</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#estimation"><i class="fa fa-check"></i><b>16.2</b> Estimation</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#full-rank-estimation"><i class="fa fa-check"></i><b>16.2.1</b> Full rank estimation</a></li>
<li class="chapter" data-level="16.2.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#less-than-full-rank-estimation"><i class="fa fa-check"></i><b>16.2.2</b> Less than full rank estimation</a></li>
<li class="chapter" data-level="16.2.3" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#augmented-normal-equations"><i class="fa fa-check"></i><b>16.2.3</b> Augmented normal equations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="introduction-to-response-surface-methodology.html"><a href="introduction-to-response-surface-methodology.html"><i class="fa fa-check"></i><b>17</b> Introduction to Response Surface Methodology</a>
<ul>
<li class="chapter" data-level="17.1" data-path="introduction-to-response-surface-methodology.html"><a href="introduction-to-response-surface-methodology.html#fractional-factorial-experiments"><i class="fa fa-check"></i><b>17.1</b> Fractional factorial experiments</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="introduction-to-response-surface-methodology.html"><a href="introduction-to-response-surface-methodology.html#an-example-fractional-factorial-experiment"><i class="fa fa-check"></i><b>17.1.1</b> An example fractional factorial experiment</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">Chapter 14</span> Linear Regression<a href="linear-regression.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter we study multiple linear regression under the Gauss-Markov model
<span class="math display">\[Y = X\beta + \epsilon\]</span>
where <span class="math inline">\(Y\)</span> is an <span class="math inline">\(n\times 1\)</span> vector of responses, <span class="math inline">\(X\)</span> is and <span class="math inline">\(n\times(p+1)\)</span> design matrix containing a leading <span class="math inline">\(n\times 1\)</span> vector of ones (for an intercept term) and n observations on <span class="math inline">\(p\)</span> continuous covariates (also called independent, predictor, or explanatory variables). Recall the model assumes a linear relationship between the (conditional) mean of the response and the covariates , i.e., <span class="math inline">\(E(Y|X) = X\beta\)</span>, and normal random residuals with constant variance, <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span>.<br><br></p>
<p>Throughout this chapter we will use a large example data set to illustrate several concepts related to multiple linear regression models. The data set is the <em>diamonds</em> dataframe available in the <em>ggplot2</em> package in R. The data set contains prices of over 50000 diamonds, which we will predict using the size (carat) and quality (cut, color, and clarity) of the diamonds.<br><br></p>
<p>Among the concepts covered in this section are the following:
- Inference on the regression coefficients and conditional means (tests and CIs)
- Prediction intervals for a new response (the price of a new diamond)
- Interpretation of regression coefficients
- Choosing a model
- Diagnostics and residuals
- Dealing with outliers and influential observations</p>
<div id="diamonds-dataset" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Diamonds dataset<a href="linear-regression.html#diamonds-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The R package ggplot2 includes the dataframe diamonds.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="linear-regression.html#cb516-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb516-2"><a href="linear-regression.html#cb516-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(diamonds)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 10
##   carat cut       color clarity depth table price     x     y     z
##   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
## 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
## 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31
## 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
## 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75
## 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48</code></pre>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="linear-regression.html#cb518-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(diamonds<span class="sc">$</span>cut)</span></code></pre></div>
<pre><code>## [1] &quot;Fair&quot;      &quot;Good&quot;      &quot;Very Good&quot; &quot;Premium&quot;   &quot;Ideal&quot;</code></pre>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="linear-regression.html#cb520-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(diamonds<span class="sc">$</span>clarity)</span></code></pre></div>
<pre><code>## [1] &quot;I1&quot;   &quot;SI2&quot;  &quot;SI1&quot;  &quot;VS2&quot;  &quot;VS1&quot;  &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot;</code></pre>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="linear-regression.html#cb522-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(diamonds<span class="sc">$</span>color)</span></code></pre></div>
<pre><code>## [1] &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot;</code></pre>
<p>Cut, color, and clarity are coded as ordinal categorical variables. It may be helpful to use numeric versions of these; for example, cut cut.num coded as an integer 1, 2, 3, 4, or 5, rather than an ordinal variable.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="linear-regression.html#cb524-1" aria-hidden="true" tabindex="-1"></a>diamonds<span class="sc">$</span>color.num <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>color)</span>
<span id="cb524-2"><a href="linear-regression.html#cb524-2" aria-hidden="true" tabindex="-1"></a>diamonds<span class="sc">$</span>cut.num <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>cut)</span>
<span id="cb524-3"><a href="linear-regression.html#cb524-3" aria-hidden="true" tabindex="-1"></a>diamonds<span class="sc">$</span>clarity.num <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>clarity)</span></code></pre></div>
<p>The model we build may depend on what purpose it is to be used for.
- Inference: models built to be interpretable and offer best explanation of response in terms of explanatory variables
- Prediction: models built to predict the response given explanatory variables
<br><br>
Models built for inference tend to be simpler/include fewer variables than models built for prediction. For example, a model built for inference may exclude explanatory variables exhibiting collinearity, because collinearity makes the model more difficult to interpret, but collinearity generally does not make the model worse at prediction.<br />
<br><br>
First we will build a model for inference. Since we are interested in hypothesis testing as part of inference, we will check all the model assumptions. These are less important for prediction.</p>
</div>
<div id="checking-linearity" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Checking Linearity<a href="linear-regression.html#checking-linearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our multiple linear regression model assumes the conditional mean of response (price) given explanatory variables(carat, cut, color, clarity) is a linear function of those variables. For a given model we can check linearity by examining residuals versus predicted values; there should be no pattern in that plot. We can also do a preliminary check by plotting the response versus each explanatory variable (so long as there are not too many) to see whether any variables exhibit a nonlinear relationship with response. If price is non-linear in a variable, then we can use a transformation to make the relationship more linear. <br><br></p>
<p>The square root transformation on price works best in the plot of carat versus price.</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="linear-regression.html#cb525-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamonds<span class="sc">$</span>carat, diamonds<span class="sc">$</span>price)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="linear-regression.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamonds<span class="sc">$</span>carat, <span class="fu">log</span>(diamonds<span class="sc">$</span>price))</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="linear-regression.html#cb527-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamonds<span class="sc">$</span>carat, <span class="fu">sqrt</span>(diamonds<span class="sc">$</span>price))</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-3.png" width="672" /></p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="linear-regression.html#cb528-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>price<span class="sc">~</span>diamonds<span class="sc">$</span>cut)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-4.png" width="672" /></p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="linear-regression.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>price<span class="sc">~</span>diamonds<span class="sc">$</span>clarity)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-5.png" width="672" /></p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="linear-regression.html#cb530-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>price<span class="sc">~</span>diamonds<span class="sc">$</span>color)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-3-6.png" width="672" /></p>
<p>Note that if we fit a model in R with the ordinal-factor versions of the diamond quality variables R uses as many orthogonal polynomial contrasts as it can for each ordinal factor.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="linear-regression.html#cb531-1" aria-hidden="true" tabindex="-1"></a>full.lm.sqrt <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut<span class="sc">+</span>color<span class="sc">+</span>clarity, <span class="at">data =</span> diamonds)</span>
<span id="cb531-2"><a href="linear-regression.html#cb531-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(full.lm.sqrt<span class="sc">$</span>fitted.values,full.lm.sqrt<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="linear-regression.html#cb532-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut + color + clarity, data = diamonds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -150.936   -3.296   -0.478    2.824   51.690 
## 
## Coefficients:
##              Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)   0.56869    0.07737    7.351 2.00e-13 ***
## carat        64.55472    0.06659  969.371  &lt; 2e-16 ***
## cut.L         3.50254    0.11253   31.124  &lt; 2e-16 ***
## cut.Q        -1.69032    0.09912  -17.054  &lt; 2e-16 ***
## cut.C         1.10438    0.08609   12.828  &lt; 2e-16 ***
## cut^4         0.02937    0.06894    0.426    0.670    
## color.L     -13.98189    0.09802 -142.646  &lt; 2e-16 ***
## color.Q      -4.61348    0.08921  -51.712  &lt; 2e-16 ***
## color.C      -0.79061    0.08340   -9.480  &lt; 2e-16 ***
## color^4       0.67221    0.07659    8.777  &lt; 2e-16 ***
## color^5      -0.54516    0.07236   -7.533 5.02e-14 ***
## color^6      -0.09971    0.06579   -1.516    0.130    
## clarity.L    26.37314    0.17062  154.572  &lt; 2e-16 ***
## clarity.Q   -12.45929    0.15953  -78.100  &lt; 2e-16 ***
## clarity.C     6.50359    0.13658   47.619  &lt; 2e-16 ***
## clarity^4    -2.37701    0.10924  -21.761  &lt; 2e-16 ***
## clarity^5     1.60112    0.08915   17.960  &lt; 2e-16 ***
## clarity^6     0.05858    0.07768    0.754    0.451    
## clarity^7     0.49801    0.06853    7.267 3.72e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.402 on 53921 degrees of freedom
## Multiple R-squared:  0.9501, Adjusted R-squared:   0.95 
## F-statistic: 5.7e+04 on 18 and 53921 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="linear-regression.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 353387.4</code></pre>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="linear-regression.html#cb536-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 353565.3</code></pre>
<p>A simpler model re-codes the ordinal factors as integers. This model is equivalent to the above model with only linear contrasts for each factor, hence the large difference in number of parameters.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="linear-regression.html#cb538-1" aria-hidden="true" tabindex="-1"></a>full.lm.sqrt <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num, <span class="at">data =</span> diamonds)</span>
<span id="cb538-2"><a href="linear-regression.html#cb538-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(full.lm.sqrt<span class="sc">$</span>fitted.values,full.lm.sqrt<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="linear-regression.html#cb539-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut.num + color.num + clarity.num, 
##     data = diamonds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -170.349   -3.303   -0.046    3.146   46.910 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.80669    0.15774  -17.79   &lt;2e-16 ***
## carat       63.83681    0.07256  879.73   &lt;2e-16 ***
## cut.num      0.79943    0.02781   28.75   &lt;2e-16 ***
## color.num   -2.34832    0.01888 -124.38   &lt;2e-16 ***
## clarity.num  3.20315    0.02017  158.84   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.061 on 53935 degrees of freedom
## Multiple R-squared:  0.9392, Adjusted R-squared:  0.9392 
## F-statistic: 2.084e+05 on 4 and 53935 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="linear-regression.html#cb541-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 363946.8</code></pre>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="linear-regression.html#cb543-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 364000.1</code></pre>
<p>Our residuals have a pattern. We may be able to use “interactions” to obtain a better-fitting model due to <em>multicollinearity</em>—another term for correlation between covariates.</p>
</div>
<div id="multicollinearity" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Multicollinearity<a href="linear-regression.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We see that diamonds of the worst cuts, colors, and clarities are also the largest diamonds on average. That makes sense. These diamonds are not worth much of anything unless they are large; the small diamonds of poor quality didn’t even make it to the market for diamonds. There is weak multicollinearity that probably is not enough to harm the interpretability of the model if all variables are included.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="linear-regression.html#cb545-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>carat<span class="sc">~</span>diamonds<span class="sc">$</span>cut)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="linear-regression.html#cb546-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>carat<span class="sc">~</span>diamonds<span class="sc">$</span>clarity)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="linear-regression.html#cb547-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(diamonds<span class="sc">$</span>carat<span class="sc">~</span>diamonds<span class="sc">$</span>color)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-6-3.png" width="672" /></p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="linear-regression.html#cb548-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(diamonds<span class="sc">$</span>carat,<span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>cut))</span></code></pre></div>
<pre><code>## [1] -0.134967</code></pre>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="linear-regression.html#cb550-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(diamonds<span class="sc">$</span>carat,<span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>clarity))</span></code></pre></div>
<pre><code>## [1] -0.3528406</code></pre>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="linear-regression.html#cb552-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(diamonds<span class="sc">$</span>carat,<span class="fu">as.numeric</span>(diamonds<span class="sc">$</span>color))</span></code></pre></div>
<pre><code>## [1] 0.2914368</code></pre>
</div>
<div id="model-with-carat-interactions" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Model with carat interactions<a href="linear-regression.html#model-with-carat-interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Including the carat interaction with cut, color, and clarity substantially improves the fit of the model as measured by <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{adj}\)</span>, the residual error/variance, and as evidenced by the residual vs predicted plot.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="linear-regression.html#cb554-1" aria-hidden="true" tabindex="-1"></a>full.lm.sqrt <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">data =</span> diamonds)</span>
<span id="cb554-2"><a href="linear-regression.html#cb554-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut.num + color.num + clarity.num + 
##     carat * clarity.num + carat * color.num + carat * cut.num, 
##     data = diamonds)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -77.115  -2.174  -0.087   2.314  34.034 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        5.35572    0.19406   27.60  &lt; 2e-16 ***
## carat             52.84947    0.20639  256.06  &lt; 2e-16 ***
## cut.num           -0.23567    0.03783   -6.23 4.71e-10 ***
## color.num          0.45156    0.02413   18.71  &lt; 2e-16 ***
## clarity.num       -0.69980    0.02430  -28.80  &lt; 2e-16 ***
## carat:clarity.num  5.63633    0.02855  197.40  &lt; 2e-16 ***
## carat:color.num   -3.39270    0.02518 -134.74  &lt; 2e-16 ***
## carat:cut.num      1.39771    0.03972   35.19  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.771 on 53932 degrees of freedom
## Multiple R-squared:  0.9723, Adjusted R-squared:  0.9723 
## F-statistic: 2.701e+05 on 7 and 53932 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="linear-regression.html#cb556-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 321649.3</code></pre>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="linear-regression.html#cb558-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>## [1] 321729.4</code></pre>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="linear-regression.html#cb560-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(full.lm.sqrt<span class="sc">$</span>residuals)</span>
<span id="cb560-2"><a href="linear-regression.html#cb560-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(full.lm.sqrt<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="linear-regression.html#cb561-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(full.lm.sqrt<span class="sc">$</span>fitted.values,full.lm.sqrt<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="linear-regression.html#cb562-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamonds<span class="sc">$</span>carat,full.lm.sqrt<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
</div>
<div id="interpreting-the-model" class="section level2 hasAnchor" number="14.5">
<h2><span class="header-section-number">14.5</span> Interpreting the model<a href="linear-regression.html#interpreting-the-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What about interpretability? We need to be specific about our interpretations, but the (simple) interactions model is certainly interpretable. For example, the first diamond in the data has carat 0.23, it is cut, color, and clarity levels 5, 2, and 2. The interpretation of the fitted betas is as follows, if we hold cut, color, and clarity constant, and increase carat by , say, 0.05, then we will increase sqrt(price) by <span class="math inline">\(3.216264 = 0.05*(52.84947 + 5*1.39771 -2*3.39270+2*5.63633)\)</span>. For another example, suppose if that diamond were cut level 4 instead of 5? Then, it’s sqrt(price) would change by <span class="math inline">\(-0.0858033 = 0.23567 - 0.23*1.39771\)</span>.</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="linear-regression.html#cb563-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(<span class="fu">head</span>(diamonds))[<span class="dv">1</span>,]</span></code></pre></div>
<pre><code>##       carat         cut       color     clarity       depth       table 
##      &quot;0.23&quot;     &quot;Ideal&quot;         &quot;E&quot;       &quot;SI2&quot;      &quot;61.5&quot;        &quot;55&quot; 
##       price           x           y           z   color.num     cut.num 
##       &quot;326&quot;      &quot;3.95&quot;      &quot;3.98&quot;      &quot;2.43&quot;         &quot;2&quot;         &quot;5&quot; 
## clarity.num 
##         &quot;2&quot;</code></pre>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="linear-regression.html#cb565-1" aria-hidden="true" tabindex="-1"></a>full.lm.sqrt<span class="sc">$</span>fitted.values[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>##        1 
## 18.47571</code></pre>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="linear-regression.html#cb567-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">data =</span> diamonds)</span>
<span id="cb567-2"><a href="linear-regression.html#cb567-2" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span>,]</span></code></pre></div>
<pre><code>##       (Intercept)             carat           cut.num         color.num 
##              1.00              0.23              5.00              2.00 
##       clarity.num carat:clarity.num   carat:color.num     carat:cut.num 
##              2.00              0.46              0.46              1.15</code></pre>
</div>
<div id="checking-constant-variance" class="section level2 hasAnchor" number="14.6">
<h2><span class="header-section-number">14.6</span> Checking Constant Variance<a href="linear-regression.html#checking-constant-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As you may have noticed, the observed residuals ``fan out”; they increase in magnitude with the value of the predicted sqrt(price). What does this mean for our model/inferences? One consequence is that our tests/CIs for conditional means may not be valid in terms of their nominal <span class="math inline">\(\alpha\)</span> Type 1 error/coverage probability for explanatory variable values with a large estimated mean sqrt(price) value. The variance of the fitted conditional mean <span class="math inline">\(\hat\mu_{Y|x} = x^{\top}\hat\beta\)</span> is larger than estimated by <span class="math inline">\(MSEx^\top(X^\top X)^{-1}x\)</span> for large values of <span class="math inline">\(x^{\top}\hat\beta\)</span>. The reason is that the variance of <span class="math inline">\(\epsilon_i\)</span> seems to be <span class="math inline">\(\sigma^2 \times\text{carat}\)</span> rather than <span class="math inline">\(\sigma^2\)</span>.<br><br></p>
<p>We will come back to this topic later to study what can be done about non-constant variance.</p>
</div>
<div id="normality" class="section level2 hasAnchor" number="14.7">
<h2><span class="header-section-number">14.7</span> Normality<a href="linear-regression.html#normality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How bad is this? The residuals are ``normal” for about the middle 90-95 % of their distribution. There are some very extreme residuals; these are diamonds with prices poorly predicted by the model. We should be cautious when interpreting the model for these diamonds. We have likely missed some important information about their prices.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="linear-regression.html#cb569-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(full.lm.sqrt<span class="sc">$</span>residuals<span class="sc">/</span><span class="fu">sd</span>(full.lm.sqrt<span class="sc">$</span>residuals))</span>
<span id="cb569-2"><a href="linear-regression.html#cb569-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(full.lm.sqrt<span class="sc">$</span>residuals<span class="sc">/</span><span class="fu">sd</span>(full.lm.sqrt<span class="sc">$</span>residuals), <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span>
<span id="cb569-3"><a href="linear-regression.html#cb569-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb569-4"><a href="linear-regression.html#cb569-4" aria-hidden="true" tabindex="-1"></a>sorted <span class="ot">&lt;-</span> <span class="fu">sort</span>(full.lm.sqrt<span class="sc">$</span>residuals)</span>
<span id="cb569-5"><a href="linear-regression.html#cb569-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="sc">-</span><span class="fl">1.644854</span>, <span class="sc">-</span><span class="fl">1.381829</span>, <span class="at">col =</span> <span class="st">&#39;red&#39;</span>, <span class="at">pch =</span> <span class="st">&#39;*&#39;</span>)</span>
<span id="cb569-6"><a href="linear-regression.html#cb569-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="sc">-</span><span class="fl">1.96</span>, <span class="sc">-</span><span class="fl">2.067442</span> , <span class="at">col =</span> <span class="st">&#39;red&#39;</span>, <span class="at">pch =</span> <span class="st">&#39;*&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="addressing-non-constant-variance-using-wls" class="section level2 hasAnchor" number="14.8">
<h2><span class="header-section-number">14.8</span> Addressing non-constant variance using WLS<a href="linear-regression.html#addressing-non-constant-variance-using-wls" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our increasing variance in residuals as a function of predicted price seems to have to do with carat. We can perform a weighted least squares estimation by weighting the observations by carat. The idea is that prices of diamonds with larger carat size have more variance, so we should downweight these observations compared to observations of smaller carat diamonds.<br />
<br><br>
In R we can do this by using the lm() function with the wt option equal to 1/carat.
<br><br>
Using weights does not “correct” the non-constant variance; rather, we are incorporating the non-constant variance into the model.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="linear-regression.html#cb570-1" aria-hidden="true" tabindex="-1"></a>get.weights <span class="ot">&lt;-</span> <span class="fu">lm</span>(full.lm.sqrt<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> full.lm.sqrt<span class="sc">$</span>fitted.values)</span>
<span id="cb570-2"><a href="linear-regression.html#cb570-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb570-3"><a href="linear-regression.html#cb570-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb570-4"><a href="linear-regression.html#cb570-4" aria-hidden="true" tabindex="-1"></a>wls <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">weights  =</span> <span class="dv">1</span><span class="sc">/</span>diamonds<span class="sc">$</span>carat, <span class="at">data =</span> diamonds)</span>
<span id="cb570-5"><a href="linear-regression.html#cb570-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wls)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut.num + color.num + clarity.num + 
##     carat * clarity.num + carat * color.num + carat * cut.num, 
##     data = diamonds, weights = 1/diamonds$carat)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -45.644  -2.700  -0.031   2.874  42.921 
## 
## Coefficients:
##                   Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)        3.82491    0.13972   27.376  &lt; 2e-16 ***
## carat             54.59492    0.19776  276.068  &lt; 2e-16 ***
## cut.num           -0.08956    0.02699   -3.318 0.000907 ***
## color.num          0.15453    0.01704    9.067  &lt; 2e-16 ***
## clarity.num       -0.32964    0.01717  -19.199  &lt; 2e-16 ***
## carat:clarity.num  5.21950    0.02755  189.471  &lt; 2e-16 ***
## carat:color.num   -3.11506    0.02404 -129.588  &lt; 2e-16 ***
## carat:cut.num      1.23565    0.03821   32.339  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.583 on 53932 degrees of freedom
## Multiple R-squared:  0.9764, Adjusted R-squared:  0.9764 
## F-statistic: 3.192e+05 on 7 and 53932 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="linear-regression.html#cb572-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(wls)</span></code></pre></div>
<pre><code>## [1] 296006.8</code></pre>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="linear-regression.html#cb574-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(wls)</span></code></pre></div>
<pre><code>## [1] 296086.8</code></pre>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="linear-regression.html#cb576-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(wls<span class="sc">$</span>residuals)</span>
<span id="cb576-2"><a href="linear-regression.html#cb576-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(wls<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="linear-regression.html#cb577-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values,wls<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="linear-regression.html#cb578-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(diamonds<span class="sc">$</span>carat,wls<span class="sc">$</span>residuals) </span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-10-3.png" width="672" /></p>
</div>
<div id="inferences-using-wls" class="section level2 hasAnchor" number="14.9">
<h2><span class="header-section-number">14.9</span> Inferences using WLS<a href="linear-regression.html#inferences-using-wls" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s compare confidence/prediction intervals using the WLS and OLS models. The WLS inferences are more “honest” than the OLS inferences because the OLS model’s assumption of constant variance is clearly violated. But, let’s see what practical difference it makes.
<br><br>
1. 95% CI for the carat beta<br>
2. 95% CI for mean price for carat = 1.5, cut = 3, clarity = 5, color = 2. This is a fairly large diamond of good quality.<br>
3. 95% Prediction interval for the same.</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="linear-regression.html#cb579-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">data=</span> diamonds)</span>
<span id="cb579-2"><a href="linear-regression.html#cb579-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(diamonds<span class="sc">$</span>price)</span>
<span id="cb579-3"><a href="linear-regression.html#cb579-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb579-4"><a href="linear-regression.html#cb579-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb579-5"><a href="linear-regression.html#cb579-5" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>diamonds<span class="sc">$</span>carat)</span>
<span id="cb579-6"><a href="linear-regression.html#cb579-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb579-7"><a href="linear-regression.html#cb579-7" aria-hidden="true" tabindex="-1"></a><span class="do">### OLS inferences</span></span>
<span id="cb579-8"><a href="linear-regression.html#cb579-8" aria-hidden="true" tabindex="-1"></a>XX.inv <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)</span>
<span id="cb579-9"><a href="linear-regression.html#cb579-9" aria-hidden="true" tabindex="-1"></a>beta.hat.ols <span class="ot">&lt;-</span> XX.inv<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span>
<span id="cb579-10"><a href="linear-regression.html#cb579-10" aria-hidden="true" tabindex="-1"></a>beta.hat.ols</span></code></pre></div>
<pre><code>##                         [,1]
## (Intercept)        5.3557161
## carat             52.8494746
## cut.num           -0.2356664
## color.num          0.4515624
## clarity.num       -0.6998048
## carat:clarity.num  5.6363319
## carat:color.num   -3.3927023
## carat:cut.num      1.3977070</code></pre>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="linear-regression.html#cb581-1" aria-hidden="true" tabindex="-1"></a>MSE.ols <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>(n<span class="sc">-</span>p))<span class="sc">*</span><span class="fu">sum</span>((Y <span class="sc">-</span> X<span class="sc">%*%</span>beta.hat.ols)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb581-2"><a href="linear-regression.html#cb581-2" aria-hidden="true" tabindex="-1"></a>MSE.ols</span></code></pre></div>
<pre><code>## [1] 22.76062</code></pre>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="linear-regression.html#cb583-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for carat beta</span></span>
<span id="cb583-2"><a href="linear-regression.html#cb583-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(beta.hat.ols[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span>XX.inv[<span class="dv">2</span>,<span class="dv">2</span>]), beta.hat.ols[<span class="dv">2</span>] <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span>XX.inv[<span class="dv">2</span>,<span class="dv">2</span>]))</span></code></pre></div>
<pre><code>## [1] 52.44495 53.25400</code></pre>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="linear-regression.html#cb585-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for mean price of that diamond</span></span>
<span id="cb585-2"><a href="linear-regression.html#cb585-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="fl">1.5</span><span class="sc">*</span><span class="dv">5</span>, <span class="fl">1.5</span><span class="sc">*</span><span class="dv">2</span>, <span class="fl">1.5</span><span class="sc">*</span><span class="dv">3</span>), <span class="dv">8</span>,<span class="dv">1</span>)</span>
<span id="cb585-3"><a href="linear-regression.html#cb585-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.ols <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XX.inv<span class="sc">%*%</span>x), <span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.ols <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XX.inv<span class="sc">%*%</span>x))</span></code></pre></div>
<pre><code>## [1] 119.5424 119.8798</code></pre>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="linear-regression.html#cb587-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PI</span></span>
<span id="cb587-2"><a href="linear-regression.html#cb587-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.ols <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XX.inv<span class="sc">%*%</span>x)), <span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.ols <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.ols<span class="sc">*</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XX.inv<span class="sc">%*%</span>x)))</span></code></pre></div>
<pre><code>## [1] 110.3587 129.0634</code></pre>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="linear-regression.html#cb589-1" aria-hidden="true" tabindex="-1"></a><span class="do">### WLS inferences</span></span>
<span id="cb589-2"><a href="linear-regression.html#cb589-2" aria-hidden="true" tabindex="-1"></a>XW <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>,p,n)</span>
<span id="cb589-3"><a href="linear-regression.html#cb589-3" aria-hidden="true" tabindex="-1"></a>tX <span class="ot">&lt;-</span> <span class="fu">t</span>(X)</span>
<span id="cb589-4"><a href="linear-regression.html#cb589-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n){</span>
<span id="cb589-5"><a href="linear-regression.html#cb589-5" aria-hidden="true" tabindex="-1"></a>  Wj <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>,n,<span class="dv">1</span>)</span>
<span id="cb589-6"><a href="linear-regression.html#cb589-6" aria-hidden="true" tabindex="-1"></a>  Wj[j] <span class="ot">&lt;-</span> W[j]</span>
<span id="cb589-7"><a href="linear-regression.html#cb589-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p){</span>
<span id="cb589-8"><a href="linear-regression.html#cb589-8" aria-hidden="true" tabindex="-1"></a>  XW[i,j] <span class="ot">&lt;-</span> tX[i,]<span class="sc">%*%</span>Wj</span>
<span id="cb589-9"><a href="linear-regression.html#cb589-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb589-10"><a href="linear-regression.html#cb589-10" aria-hidden="true" tabindex="-1"></a>}  </span>
<span id="cb589-11"><a href="linear-regression.html#cb589-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb589-12"><a href="linear-regression.html#cb589-12" aria-hidden="true" tabindex="-1"></a>XWX.inv <span class="ot">&lt;-</span> <span class="fu">solve</span>(XW<span class="sc">%*%</span>X)</span>
<span id="cb589-13"><a href="linear-regression.html#cb589-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb589-14"><a href="linear-regression.html#cb589-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb589-15"><a href="linear-regression.html#cb589-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb589-16"><a href="linear-regression.html#cb589-16" aria-hidden="true" tabindex="-1"></a>beta.hat.wls <span class="ot">&lt;-</span> XWX.inv<span class="sc">%*%</span>XW<span class="sc">%*%</span>Y</span>
<span id="cb589-17"><a href="linear-regression.html#cb589-17" aria-hidden="true" tabindex="-1"></a>beta.hat.wls</span></code></pre></div>
<pre><code>##                          [,1]
## (Intercept)        3.82491459
## carat             54.59491779
## cut.num           -0.08955524
## color.num          0.15452811
## clarity.num       -0.32964027
## carat:clarity.num  5.21950075
## carat:color.num   -3.11506184
## carat:cut.num      1.23564947</code></pre>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="linear-regression.html#cb591-1" aria-hidden="true" tabindex="-1"></a>MSE.wls <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>(n<span class="sc">-</span>p))<span class="sc">*</span><span class="fu">sum</span>(W<span class="sc">*</span>(Y <span class="sc">-</span> X<span class="sc">%*%</span>beta.hat.wls)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb591-2"><a href="linear-regression.html#cb591-2" aria-hidden="true" tabindex="-1"></a>MSE.wls</span></code></pre></div>
<pre><code>## [1] 21.00181</code></pre>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="linear-regression.html#cb593-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(beta.hat.wls[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>XWX.inv[<span class="dv">2</span>,<span class="dv">2</span>]), beta.hat.wls[<span class="dv">2</span>] <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>XWX.inv[<span class="dv">2</span>,<span class="dv">2</span>]))</span></code></pre></div>
<pre><code>##    carat    carat 
## 54.20731 54.98253</code></pre>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="linear-regression.html#cb595-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x), <span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x))</span></code></pre></div>
<pre><code>## [1] 119.2983 119.6437</code></pre>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="linear-regression.html#cb597-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x)), <span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x)))</span></code></pre></div>
<pre><code>## [1] 110.4870 128.4549</code></pre>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="linear-regression.html#cb599-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>(<span class="fl">1.5</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x)), <span class="fu">t</span>(x)<span class="sc">%*%</span>beta.hat.wls <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n<span class="sc">-</span>p)<span class="sc">*</span><span class="fu">sqrt</span>(MSE.wls<span class="sc">*</span>(<span class="fl">1.5</span><span class="sc">+</span><span class="fu">t</span>(x)<span class="sc">%*%</span>XWX.inv<span class="sc">%*%</span>x)))</span></code></pre></div>
<pre><code>## [1] 108.4686 130.4733</code></pre>
</div>
<div id="using-built-in-functions-in-r-for-inference" class="section level2 hasAnchor" number="14.10">
<h2><span class="header-section-number">14.10</span> Using built-in functions in R for inference<a href="linear-regression.html#using-built-in-functions-in-r-for-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Make sure to use the weights option in predict.lm for predicting a new response based on the WLS regression model.</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="linear-regression.html#cb601-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(full.lm.sqrt)</span></code></pre></div>
<pre><code>##                        2.5 %     97.5 %
## (Intercept)        4.9753505  5.7360816
## carat             52.4449462 53.2540030
## cut.num           -0.3098125 -0.1615203
## color.num          0.4042664  0.4988584
## clarity.num       -0.7474246 -0.6521849
## carat:clarity.num  5.5803670  5.6922968
## carat:color.num   -3.4420565 -3.3433480
## carat:cut.num      1.3198461  1.4755679</code></pre>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="linear-regression.html#cb603-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(wls)</span></code></pre></div>
<pre><code>##                        2.5 %      97.5 %
## (Intercept)        3.5510712  4.09875801
## carat             54.2073087 54.98252686
## cut.num           -0.1424549 -0.03665554
## color.num          0.1211232  0.18793297
## clarity.num       -0.3632933 -0.29598725
## carat:clarity.num  5.1655068  5.27349466
## carat:color.num   -3.1621770 -3.06794668
## carat:cut.num      1.1607589  1.31054002</code></pre>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="linear-regression.html#cb605-1" aria-hidden="true" tabindex="-1"></a>new.carat <span class="ot">=</span> <span class="fl">1.5</span></span>
<span id="cb605-2"><a href="linear-regression.html#cb605-2" aria-hidden="true" tabindex="-1"></a>new.cut <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb605-3"><a href="linear-regression.html#cb605-3" aria-hidden="true" tabindex="-1"></a>new.clarity <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb605-4"><a href="linear-regression.html#cb605-4" aria-hidden="true" tabindex="-1"></a>new.color <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb605-5"><a href="linear-regression.html#cb605-5" aria-hidden="true" tabindex="-1"></a>new.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">carat =</span> new.carat, <span class="at">cut.num =</span> new.cut, <span class="at">color.num =</span> new.color, <span class="at">clarity.num =</span> new.clarity)</span>
<span id="cb605-6"><a href="linear-regression.html#cb605-6" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 119.471 119.2983 119.6437</code></pre>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="linear-regression.html#cb607-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>)</span></code></pre></div>
<pre><code>## Warning in predict.lm(wls, newdata = new.data, interval = &quot;prediction&quot;): Assuming constant prediction variance even though model fit is weighted</code></pre>
<pre><code>##       fit     lwr      upr
## 1 119.471 110.487 128.4549</code></pre>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="linear-regression.html#cb610-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>, <span class="at">weights =</span> <span class="dv">1</span><span class="sc">/</span><span class="fl">1.5</span>)</span></code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 119.471 108.4686 130.4733</code></pre>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="linear-regression.html#cb612-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(full.lm.sqrt, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 119.7111 119.5424 119.8798</code></pre>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="linear-regression.html#cb614-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(full.lm.sqrt, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 119.7111 110.3587 129.0634</code></pre>
</div>
<div id="leverage-outliers-and-influence" class="section level2 hasAnchor" number="14.11">
<h2><span class="header-section-number">14.11</span> Leverage, outliers, and influence<a href="linear-regression.html#leverage-outliers-and-influence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Outlier are observations with very large residuals—the model is not good at predicting the prices of these diamonds. Large residuals may happen by chance, because the model is missing important covariate information, or because one or more assumptions are violated. Often, we consider removing observations with large residuals based on the rationale that if the model is not good at predicting those responses then those observations should not be modeled—a rationale that is, at least partially, flawed. Outlier observations are only troublesome if they have an outsized effect on the fit of the model, which may be measured by comparing <span class="math inline">\(\hat\beta\)</span> with and without the observation in question, or comparing <span class="math inline">\(\hat Y\)</span> or the <span class="math inline">\(SSE\)</span>. Observations that strongly affect the fit of the model have high <em>leverage</em>. Outliers with high leverage are <em>influential</em> points. It is only these influential points we need to worry about in terms of whether or not to include them in the model at all. On the other hand, outliers that are not influential are only important when it comes to interpreting the fit of the model at that particular observation, and not for interpreting the model in general.</p>
<div id="illustrations" class="section level3 hasAnchor" number="14.11.1">
<h3><span class="header-section-number">14.11.1</span> Illustrations<a href="linear-regression.html#illustrations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, some “cartoons” or “toy” examples. The following plots illustrate outliers, leverage, and influence using a simple linear regression on contrived data. The first plot shows a fit without any outliers. The second shows a fit with a high leverage point, that is not an outlier. The third show a pair of fitterd lines with and without a non-influential outlier (one without leverage). And, the fourth shows a plot of a pair of fitted lines with and without an influential point.</p>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="numerical-summaries-of-outliers-leverage-and-influence" class="section level2 hasAnchor" number="14.12">
<h2><span class="header-section-number">14.12</span> Numerical summaries of outliers, leverage, and influence<a href="linear-regression.html#numerical-summaries-of-outliers-leverage-and-influence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The “hat” matrix <span class="math inline">\(H = X(X^\top X)^{-1}X^\top\)</span> determines high leverage points. To see this, note that the fitted responses are given by <span class="math inline">\(\hat Y = H Y\)</span>, which are linear combinations of all the responses. We can express <span class="math inline">\(\hat Y_i\)</span>—the predicted price of the <span class="math inline">\(i^{th}\)</span> diamond—as a linear combination of the price of the <span class="math inline">\(i^{th}\)</span> diamond itself and the other <span class="math inline">\(n-1\)</span> diamonds: <span class="math inline">\(\hat Y_i = h_{ii}Y_i + \sum_{i\ne j} h_{ij}Y_j\)</span> where <span class="math inline">\(h_{ij}\)</span> is the <span class="math inline">\(ij\)</span> entry of <span class="math inline">\(H\)</span>. If we simply define leverage to be the degree to which an observed response determines its own prediction (or fitted value) then <span class="math inline">\(h_{ii}\)</span>—the diagonal of <span class="math inline">\(H\)</span>—defines leverage. Large values of leverage are <span class="math inline">\(2\)</span> to <span class="math inline">\(3\)</span> times <span class="math inline">\((p+1)/n\)</span>.</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="linear-regression.html#cb616-1" aria-hidden="true" tabindex="-1"></a>h.X <span class="ot">&lt;-</span> <span class="fu">hat</span>(X)</span>
<span id="cb616-2"><a href="linear-regression.html#cb616-2" aria-hidden="true" tabindex="-1"></a>h.X</span></code></pre></div>
<pre><code>##  [1] 0.09682659 0.08884843 0.08140573 0.07449847 0.06812665 0.06229028
##  [7] 0.05698936 0.05222389 0.04799386 0.04429928 0.04114014 0.03851646
## [13] 0.03642821 0.03487542 0.03385807 0.03337617 0.03342971 0.03401870
## [19] 0.03514314 0.03680303 0.03899836 0.04172914 0.04499536 0.04879703
## [25] 0.05313415 0.05800671 0.06341472 0.06935818 0.07583708 0.47463768</code></pre>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="linear-regression.html#cb618-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hat(model.matrix(Y~X))   # same</span></span>
<span id="cb618-2"><a href="linear-regression.html#cb618-2" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(h.X <span class="sc">&gt;</span> (<span class="dv">3</span><span class="sc">*</span><span class="dv">2</span> <span class="sc">/</span> <span class="dv">30</span>))   <span class="co"># 3(k+1)/n</span></span></code></pre></div>
<pre><code>## [1] 30</code></pre>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="linear-regression.html#cb620-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.max</span>(h.X)</span></code></pre></div>
<pre><code>## [1] 30</code></pre>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="linear-regression.html#cb622-1" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;by hand&quot;</span></span>
<span id="cb622-2"><a href="linear-regression.html#cb622-2" aria-hidden="true" tabindex="-1"></a>X.d <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">30</span>),X)</span>
<span id="cb622-3"><a href="linear-regression.html#cb622-3" aria-hidden="true" tabindex="-1"></a>hat.X <span class="ot">&lt;-</span> X.d<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X.d)<span class="sc">%*%</span>X.d)<span class="sc">%*%</span><span class="fu">t</span>(X.d)  <span class="co"># n by n matrix</span></span>
<span id="cb622-4"><a href="linear-regression.html#cb622-4" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(hat.X)</span></code></pre></div>
<pre><code>##  [1] 0.09682659 0.08884843 0.08140573 0.07449847 0.06812665 0.06229028
##  [7] 0.05698936 0.05222389 0.04799386 0.04429928 0.04114014 0.03851646
## [13] 0.03642821 0.03487542 0.03385807 0.03337617 0.03342971 0.03401870
## [19] 0.03514314 0.03680303 0.03899836 0.04172914 0.04499536 0.04879703
## [25] 0.05313415 0.05800671 0.06341472 0.06935818 0.07583708 0.47463768</code></pre>
<p>Outliers are defined by large residuals. In multiple linear regression there is more than one way to define residuals and studentized residuals. “Studentized” refers to normalizing residuals by the estimated standard deviation. Internally studentized residuals include each observation in the calculation of the estimated standard deviation whereas externally studentized residuals ignore the corresponding observation when computing the estimated standard deviation for normalization. Residuals between 2 and 3 are borderline outliers, while residuals above 3 are generally considered to be outliers.</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="linear-regression.html#cb624-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, my.lm<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="linear-regression.html#cb625-1" aria-hidden="true" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> <span class="fu">sum</span>(my.lm<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">30-2</span>)</span>
<span id="cb625-2"><a href="linear-regression.html#cb625-2" aria-hidden="true" tabindex="-1"></a>int.stud.resids <span class="ot">&lt;-</span> my.lm<span class="sc">$</span>residuals <span class="sc">/</span> <span class="fu">sqrt</span>(MSE <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>h.X))</span>
<span id="cb625-3"><a href="linear-regression.html#cb625-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, int.stud.resids)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="linear-regression.html#cb626-1" aria-hidden="true" tabindex="-1"></a>MSE.i <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">30</span>)</span>
<span id="cb626-2"><a href="linear-regression.html#cb626-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>){</span>
<span id="cb626-3"><a href="linear-regression.html#cb626-3" aria-hidden="true" tabindex="-1"></a>MSE.i[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(my.lm<span class="sc">$</span>residuals[<span class="sc">-</span>i]<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">30-2-1</span>)</span>
<span id="cb626-4"><a href="linear-regression.html#cb626-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb626-5"><a href="linear-regression.html#cb626-5" aria-hidden="true" tabindex="-1"></a>ext.stud.resids <span class="ot">&lt;-</span> my.lm<span class="sc">$</span>residuals <span class="sc">/</span> <span class="fu">sqrt</span>(MSE.i <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>h.X))</span>
<span id="cb626-6"><a href="linear-regression.html#cb626-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, ext.stud.resids)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-18-3.png" width="672" /></p>
<p>An observation’s <em>influence</em> can be defined by the degree to which is affects the model through <span class="math inline">\(\hat\beta\)</span> or <span class="math inline">\(\hat Y\)</span>. Cook’s D and DfFits measure the fit of the model, which is related to residuals and fitted/predicted values, whereas DfBetas measures the change in the fitted coefficients with or without the given observation. There is considerable disagreement over what constitutes a “large” value of Cook’s distance. Some practitioners use the cutoff 1, which is supported by the following argument. The cook’s distance for a single observation is a random variable distributed approximately according to an <span class="math inline">\(F\)</span> distribution with numerator and denominator degrees of freedom <span class="math inline">\(p+1\)</span> and <span class="math inline">\(n\)</span>. If we take the median of this distribution to be the cutoff for large Cook’s distance, then the cutoff is approximately <span class="math inline">\(\frac{n}{3n-2}\frac{3(p+1)-2}{p}\approx 1\)</span> if <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is moderate, but it can be substantially different from 1 otherwise. The Cook’s distance plotting function in the olsrr package uses the cutoff 4/n, which, of course, is quite different. DfFits greater than <span class="math inline">\(2\sqrt{p/n}\)</span> and DfBetas greater than <span class="math inline">\(2/\sqrt{n}\)</span> are generally considered to be large and indicate outsized influence.</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="linear-regression.html#cb627-1" aria-hidden="true" tabindex="-1"></a><span class="co"># base r functions</span></span>
<span id="cb627-2"><a href="linear-regression.html#cb627-2" aria-hidden="true" tabindex="-1"></a>cd <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(my.lm)</span>
<span id="cb627-3"><a href="linear-regression.html#cb627-3" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(cd <span class="sc">&gt;</span> (<span class="dv">30</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">*</span><span class="dv">30-2</span>))<span class="sc">*</span>(<span class="dv">3</span><span class="sc">*</span><span class="dv">2-2</span>)<span class="sc">/</span>(<span class="dv">2</span>))</span></code></pre></div>
<pre><code>## 30 
## 30</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="linear-regression.html#cb629-1" aria-hidden="true" tabindex="-1"></a>dfb <span class="ot">&lt;-</span> <span class="fu">dfbetas</span>(my.lm)</span>
<span id="cb629-2"><a href="linear-regression.html#cb629-2" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(dfb <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">30</span>))</span></code></pre></div>
<pre><code>## [1] 30</code></pre>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="linear-regression.html#cb631-1" aria-hidden="true" tabindex="-1"></a>dff <span class="ot">&lt;-</span> <span class="fu">dffits</span>(my.lm)</span>
<span id="cb631-2"><a href="linear-regression.html#cb631-2" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(dff <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">/</span><span class="dv">30</span>))</span></code></pre></div>
<pre><code>## 26 
## 26</code></pre>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="linear-regression.html#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(olsrr)</span></code></pre></div>
<pre><code>## Warning: package &#39;olsrr&#39; was built under R version 4.2.2</code></pre>
<pre><code>## 
## Attaching package: &#39;olsrr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:datasets&#39;:
## 
##     rivers</code></pre>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="linear-regression.html#cb637-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ols_plot_cooksd_bar</span>(my.lm)  <span class="co"># lol</span></span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="linear-regression.html#cb638-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ols_plot_dfbetas</span>(my.lm)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="linear-regression.html#cb639-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ols_plot_dffits</span>(my.lm)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-19-3.png" width="672" /></p>
</div>
<div id="leverage-diamonds-model" class="section level2 hasAnchor" number="14.13">
<h2><span class="header-section-number">14.13</span> Leverage, Diamonds model<a href="linear-regression.html#leverage-diamonds-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="linear-regression.html#cb640-1" aria-hidden="true" tabindex="-1"></a>wls <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">weights  =</span> <span class="dv">1</span><span class="sc">/</span>diamonds<span class="sc">$</span>carat, <span class="at">data =</span> diamonds)</span>
<span id="cb640-2"><a href="linear-regression.html#cb640-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-3"><a href="linear-regression.html#cb640-3" aria-hidden="true" tabindex="-1"></a>MSE.wls <span class="ot">&lt;-</span> <span class="fu">sum</span>((wls<span class="sc">$</span>residuals)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">nrow</span>(diamonds)<span class="sc">-</span><span class="dv">8</span>)</span>
<span id="cb640-4"><a href="linear-regression.html#cb640-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-5"><a href="linear-regression.html#cb640-5" aria-hidden="true" tabindex="-1"></a>leverages.lm <span class="ot">&lt;-</span> <span class="fu">lm.influence</span>(wls)<span class="sc">$</span>hat</span>
<span id="cb640-6"><a href="linear-regression.html#cb640-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-7"><a href="linear-regression.html#cb640-7" aria-hidden="true" tabindex="-1"></a><span class="co"># k + 1  = 8 fitted regression coefficients</span></span>
<span id="cb640-8"><a href="linear-regression.html#cb640-8" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 53940</span></span>
<span id="cb640-9"><a href="linear-regression.html#cb640-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-10"><a href="linear-regression.html#cb640-10" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> <span class="sc">*</span> <span class="dv">8</span><span class="sc">/</span><span class="dv">53940</span></span></code></pre></div>
<pre><code>## [1] 0.0004449388</code></pre>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="linear-regression.html#cb642-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>)</span></code></pre></div>
<pre><code>## [1] 1248</code></pre>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb644-1"><a href="linear-regression.html#cb644-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>) <span class="sc">/</span> <span class="dv">53940</span></span></code></pre></div>
<pre><code>## [1] 0.02313682</code></pre>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="linear-regression.html#cb646-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values[leverages.lm <span class="sc">&lt;</span> <span class="fl">0.00045</span>], wls<span class="sc">$</span>residuals[leverages.lm <span class="sc">&lt;</span> <span class="fl">0.00045</span>])</span>
<span id="cb646-2"><a href="linear-regression.html#cb646-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>], wls<span class="sc">$</span>residuals[leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>], <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div id="outliers-diamonds-model" class="section level3 hasAnchor" number="14.13.1">
<h3><span class="header-section-number">14.13.1</span> Outliers, diamonds model<a href="linear-regression.html#outliers-diamonds-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="linear-regression.html#cb647-1" aria-hidden="true" tabindex="-1"></a>int.stud.resids <span class="ot">&lt;-</span> wls<span class="sc">$</span>residuals <span class="sc">/</span> <span class="fu">sqrt</span>(MSE.wls <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>leverages.lm)<span class="sc">*</span>(diamonds<span class="sc">$</span>carat))</span>
<span id="cb647-2"><a href="linear-regression.html#cb647-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, int.stud.resids)</span>
<span id="cb647-3"><a href="linear-regression.html#cb647-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(int.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span>], int.stud.resids[<span class="fu">abs</span>(int.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span>], <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb648-1"><a href="linear-regression.html#cb648-1" aria-hidden="true" tabindex="-1"></a>MSE.i <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">53940</span>)</span>
<span id="cb648-2"><a href="linear-regression.html#cb648-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">53940</span>){</span>
<span id="cb648-3"><a href="linear-regression.html#cb648-3" aria-hidden="true" tabindex="-1"></a>MSE.i[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(wls<span class="sc">$</span>residuals[<span class="sc">-</span>i]<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">53940-8-1</span>)</span>
<span id="cb648-4"><a href="linear-regression.html#cb648-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb648-5"><a href="linear-regression.html#cb648-5" aria-hidden="true" tabindex="-1"></a>ext.stud.resids <span class="ot">&lt;-</span> wls<span class="sc">$</span>residuals <span class="sc">/</span> <span class="fu">sqrt</span>(MSE.i <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>leverages.lm)<span class="sc">*</span>(diamonds<span class="sc">$</span>carat))</span>
<span id="cb648-6"><a href="linear-regression.html#cb648-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb648-7"><a href="linear-regression.html#cb648-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span>], ext.stud.resids[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span>], <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
</div>
<div id="plotting-outliers-with-leverage" class="section level3 hasAnchor" number="14.13.2">
<h3><span class="header-section-number">14.13.2</span> plotting outliers with leverage<a href="linear-regression.html#plotting-outliers-with-leverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb649-1"><a href="linear-regression.html#cb649-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb649-2"><a href="linear-regression.html#cb649-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span> <span class="sc">&amp;</span> leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>], ext.stud.resids[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span> <span class="sc">&amp;</span> leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>], <span class="at">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="linear-regression.html#cb650-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span> <span class="sc">&amp;</span> leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>])</span></code></pre></div>
<pre><code>## [1] 179</code></pre>
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb652-1"><a href="linear-regression.html#cb652-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span> <span class="sc">&amp;</span> leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>])<span class="sc">/</span><span class="dv">53940</span></span></code></pre></div>
<pre><code>## [1] 0.003318502</code></pre>
</div>
<div id="cooks-distance-df-betas-df-fits" class="section level3 hasAnchor" number="14.13.3">
<h3><span class="header-section-number">14.13.3</span> Cook’s distance, DF betas, DF fits<a href="linear-regression.html#cooks-distance-df-betas-df-fits" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb654-1"><a href="linear-regression.html#cb654-1" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">53940</span></span>
<span id="cb654-2"><a href="linear-regression.html#cb654-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(wls<span class="sc">$</span>coefficients)</span>
<span id="cb654-3"><a href="linear-regression.html#cb654-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb654-4"><a href="linear-regression.html#cb654-4" aria-hidden="true" tabindex="-1"></a><span class="co"># base r functions</span></span>
<span id="cb654-5"><a href="linear-regression.html#cb654-5" aria-hidden="true" tabindex="-1"></a>cd <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(wls, <span class="at">weights  =</span> <span class="dv">1</span><span class="sc">/</span>diamonds<span class="sc">$</span>carat)</span>
<span id="cb654-6"><a href="linear-regression.html#cb654-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(cd <span class="sc">&gt;</span> (((n)<span class="sc">/</span>(<span class="dv">3</span><span class="sc">*</span>n<span class="dv">-2</span>)) <span class="sc">*</span> ((<span class="dv">3</span><span class="sc">*</span>p<span class="dv">-2</span>)<span class="sc">/</span>(p))) )</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb656-1"><a href="linear-regression.html#cb656-1" aria-hidden="true" tabindex="-1"></a>dfb <span class="ot">&lt;-</span> <span class="fu">dfbetas</span>(wls, <span class="at">weights  =</span> <span class="dv">1</span><span class="sc">/</span>diamonds<span class="sc">$</span>carat)</span>
<span id="cb656-2"><a href="linear-regression.html#cb656-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(dfb) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n))</span></code></pre></div>
<pre><code>## [1] 25530</code></pre>
<div class="sourceCode" id="cb658"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb658-1"><a href="linear-regression.html#cb658-1" aria-hidden="true" tabindex="-1"></a>dff <span class="ot">&lt;-</span> <span class="fu">dffits</span>(wls)</span>
<span id="cb658-2"><a href="linear-regression.html#cb658-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(dff) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n))</span></code></pre></div>
<pre><code>## [1] 3201</code></pre>
<div class="sourceCode" id="cb660"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb660-1"><a href="linear-regression.html#cb660-1" aria-hidden="true" tabindex="-1"></a>my.dff <span class="ot">&lt;-</span> ext.stud.resids<span class="sc">*</span><span class="fu">sqrt</span>(leverages.lm<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>leverages.lm))</span>
<span id="cb660-2"><a href="linear-regression.html#cb660-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(my.dff <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n))</span></code></pre></div>
<pre><code>## [1] 1131</code></pre>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb662-1"><a href="linear-regression.html#cb662-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb662-2"><a href="linear-regression.html#cb662-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[cd <span class="sc">&gt;</span> (((n)<span class="sc">/</span>(<span class="dv">3</span><span class="sc">*</span>n<span class="dv">-2</span>)) <span class="sc">*</span> ((<span class="dv">3</span><span class="sc">*</span>p<span class="dv">-2</span>)<span class="sc">/</span>(p)))], ext.stud.resids[cd <span class="sc">&gt;</span> (((n)<span class="sc">/</span>(<span class="dv">3</span><span class="sc">*</span>n<span class="dv">-2</span>)) <span class="sc">*</span> ((<span class="dv">3</span><span class="sc">*</span>p<span class="dv">-2</span>)<span class="sc">/</span>(p)))], <span class="at">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb663-1"><a href="linear-regression.html#cb663-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb663-2"><a href="linear-regression.html#cb663-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(dfb) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n)], ext.stud.resids[<span class="fu">abs</span>(dfb) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n)], <span class="at">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb664-1"><a href="linear-regression.html#cb664-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb664-2"><a href="linear-regression.html#cb664-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(dff) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n)], ext.stud.resids[<span class="fu">abs</span>(dff) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n)], <span class="at">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-23-3.png" width="672" /></p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb665-1"><a href="linear-regression.html#cb665-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(wls<span class="sc">$</span>fitted.values, ext.stud.resids)</span>
<span id="cb665-2"><a href="linear-regression.html#cb665-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(wls<span class="sc">$</span>fitted.values[<span class="fu">abs</span>(my.dff) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n)], ext.stud.resids[<span class="fu">abs</span>(my.dff) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(p<span class="sc">/</span>n)], <span class="at">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="13-MultipleLinearRegression_files/figure-html/unnamed-chunk-23-4.png" width="672" /></p>
</div>
<div id="model-fit-without-high-leverage-outliers" class="section level3 hasAnchor" number="14.13.4">
<h3><span class="header-section-number">14.13.4</span> Model fit without high leverage outliers<a href="linear-regression.html#model-fit-without-high-leverage-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb666"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb666-1"><a href="linear-regression.html#cb666-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wls)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut.num + color.num + clarity.num + 
##     carat * clarity.num + carat * color.num + carat * cut.num, 
##     data = diamonds, weights = 1/diamonds$carat)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -45.644  -2.700  -0.031   2.874  42.921 
## 
## Coefficients:
##                   Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)        3.82491    0.13972   27.376  &lt; 2e-16 ***
## carat             54.59492    0.19776  276.068  &lt; 2e-16 ***
## cut.num           -0.08956    0.02699   -3.318 0.000907 ***
## color.num          0.15453    0.01704    9.067  &lt; 2e-16 ***
## clarity.num       -0.32964    0.01717  -19.199  &lt; 2e-16 ***
## carat:clarity.num  5.21950    0.02755  189.471  &lt; 2e-16 ***
## carat:color.num   -3.11506    0.02404 -129.588  &lt; 2e-16 ***
## carat:cut.num      1.23565    0.03821   32.339  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.583 on 53932 degrees of freedom
## Multiple R-squared:  0.9764, Adjusted R-squared:  0.9764 
## F-statistic: 3.192e+05 on 7 and 53932 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb668-1"><a href="linear-regression.html#cb668-1" aria-hidden="true" tabindex="-1"></a>diamonds2 <span class="ot">&lt;-</span> diamonds[<span class="sc">!</span>(<span class="fu">abs</span>(ext.stud.resids)<span class="sc">&gt;</span><span class="dv">3</span> <span class="sc">&amp;</span> leverages.lm <span class="sc">&gt;</span> <span class="fl">0.00045</span>),]</span>
<span id="cb668-2"><a href="linear-regression.html#cb668-2" aria-hidden="true" tabindex="-1"></a>wls2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(price)<span class="sc">~</span>carat<span class="sc">+</span>cut.num<span class="sc">+</span>color.num<span class="sc">+</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>clarity.num <span class="sc">+</span> carat<span class="sc">*</span>color.num <span class="sc">+</span>carat<span class="sc">*</span>cut.num, <span class="at">weights  =</span> <span class="dv">1</span><span class="sc">/</span>diamonds2<span class="sc">$</span>carat, <span class="at">data =</span> diamonds2)</span>
<span id="cb668-3"><a href="linear-regression.html#cb668-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb668-4"><a href="linear-regression.html#cb668-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wls2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sqrt(price) ~ carat + cut.num + color.num + clarity.num + 
##     carat * clarity.num + carat * color.num + carat * cut.num, 
##     data = diamonds2, weights = 1/diamonds2$carat)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.735  -2.691  -0.061   2.795  38.298 
## 
## Coefficients:
##                    Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept)        3.066818   0.136417   22.481  &lt; 2e-16 ***
## carat             55.782304   0.195343  285.561  &lt; 2e-16 ***
## cut.num            0.009513   0.026228    0.363    0.717    
## color.num          0.122563   0.016485    7.435 1.06e-13 ***
## clarity.num       -0.253231   0.016751  -15.118  &lt; 2e-16 ***
## carat:clarity.num  5.099607   0.027196  187.511  &lt; 2e-16 ***
## carat:color.num   -3.067312   0.023386 -131.159  &lt; 2e-16 ***
## carat:cut.num      1.080849   0.037405   28.896  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.403 on 53753 degrees of freedom
## Multiple R-squared:  0.9781, Adjusted R-squared:  0.9781 
## F-statistic: 3.437e+05 on 7 and 53753 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="linear-regression.html#cb670-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(wls)</span></code></pre></div>
<pre><code>##                        2.5 %      97.5 %
## (Intercept)        3.5510712  4.09875801
## carat             54.2073087 54.98252686
## cut.num           -0.1424549 -0.03665554
## color.num          0.1211232  0.18793297
## clarity.num       -0.3632933 -0.29598725
## carat:clarity.num  5.1655068  5.27349466
## carat:color.num   -3.1621770 -3.06794668
## carat:cut.num      1.1607589  1.31054002</code></pre>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="linear-regression.html#cb672-1" aria-hidden="true" tabindex="-1"></a>new.carat <span class="ot">=</span> <span class="fl">1.5</span></span>
<span id="cb672-2"><a href="linear-regression.html#cb672-2" aria-hidden="true" tabindex="-1"></a>new.cut <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb672-3"><a href="linear-regression.html#cb672-3" aria-hidden="true" tabindex="-1"></a>new.clarity <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb672-4"><a href="linear-regression.html#cb672-4" aria-hidden="true" tabindex="-1"></a>new.color <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb672-5"><a href="linear-regression.html#cb672-5" aria-hidden="true" tabindex="-1"></a>new.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">carat =</span> new.carat, <span class="at">cut.num =</span> new.cut, <span class="at">color.num =</span> new.color, <span class="at">clarity.num =</span> new.clarity)</span>
<span id="cb672-6"><a href="linear-regression.html#cb672-6" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 119.471 119.2983 119.6437</code></pre>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb674-1"><a href="linear-regression.html#cb674-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>, <span class="at">weights =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<pre><code>##       fit      lwr     upr
## 1 119.471 112.1349 126.807</code></pre>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="linear-regression.html#cb676-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(wls2)</span></code></pre></div>
<pre><code>##                         2.5 %      97.5 %
## (Intercept)        2.79943898  3.33419658
## carat             55.39943076 56.16517756
## cut.num           -0.04189338  0.06092028
## color.num          0.09025309  0.15487319
## clarity.num       -0.28606191 -0.22039954
## carat:clarity.num  5.04630203  5.15291189
## carat:color.num   -3.11314867 -3.02147445
## carat:cut.num      1.00753474  1.15416254</code></pre>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="linear-regression.html#cb678-1" aria-hidden="true" tabindex="-1"></a>new.carat <span class="ot">=</span> <span class="fl">1.5</span></span>
<span id="cb678-2"><a href="linear-regression.html#cb678-2" aria-hidden="true" tabindex="-1"></a>new.cut <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb678-3"><a href="linear-regression.html#cb678-3" aria-hidden="true" tabindex="-1"></a>new.clarity <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb678-4"><a href="linear-regression.html#cb678-4" aria-hidden="true" tabindex="-1"></a>new.color <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb678-5"><a href="linear-regression.html#cb678-5" aria-hidden="true" tabindex="-1"></a>new.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">carat =</span> new.carat, <span class="at">cut.num =</span> new.cut, <span class="at">color.num =</span> new.color, <span class="at">clarity.num =</span> new.clarity)</span>
<span id="cb678-6"><a href="linear-regression.html#cb678-6" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls2, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 119.6567 119.4893 119.8242</code></pre>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="linear-regression.html#cb680-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(wls2, <span class="at">newdata =</span> new.data, <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>, <span class="at">weights =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 119.6567 112.6083 126.7051</code></pre>
</div>
</div>
<div id="dealing-with-highly-influential-data-points" class="section level2 hasAnchor" number="14.14">
<h2><span class="header-section-number">14.14</span> Dealing with highly influential data points<a href="linear-regression.html#dealing-with-highly-influential-data-points" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Do not simply delete and ignore influential data points.<br></li>
<li>Perform analyses both with and without the subset of points you identify as unusually influential.<br></li>
<li>Recommend further analysis be done to understand why some points are not well-explained by the model.<br>
<ol style="list-style-type: lower-alpha">
<li>Are there data-entry mistakes?<br></li>
<li>Are there important explanatory variables missing from the model?<br></li>
<li>Are these all “exceptional” cases?<br></li>
</ol></li>
</ol>
<p>Question: Suppose this model is being used as an automatic diamond pricing algorithm by a diamond seller. A new diamond is “fed” into the model, and out comes a suggested price. What should be the impact of the above influence analysis on this pricing algorithm?</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="k-factorial-experiments.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="analysis-of-covariance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/13-MultipleLinearRegression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
