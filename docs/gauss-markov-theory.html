<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Gauss-Markov Theory | Applied Linear Models</title>
  <meta name="description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Gauss-Markov Theory | Applied Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Gauss-Markov Theory | Applied Linear Models" />
  
  <meta name="twitter:description" content="These are a collection of notes related to STAT 500 at Iowa State University. This is a work in progress." />
  

<meta name="author" content="Nick Syring" />


<meta name="date" content="2022-12-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="analysis-of-covariance.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html"><i class="fa fa-check"></i><b>2</b> Data Analysis and Statistical Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#data-experiments-and-studies"><i class="fa fa-check"></i><b>2.1</b> Data, Experiments, and Studies</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#james-linds-scurvy-trial"><i class="fa fa-check"></i><b>2.1.1</b> James Lind’s Scurvy Trial</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#framingham-heart-study"><i class="fa fa-check"></i><b>2.1.2</b> Framingham Heart Study</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#harris-bank-sex-pay-study"><i class="fa fa-check"></i><b>2.1.3</b> Harris Bank Sex Pay Study</a></li>
<li class="chapter" data-level="2.1.4" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#large-aggregated-data-sets"><i class="fa fa-check"></i><b>2.1.4</b> Large, Aggregated Data Sets</a></li>
<li class="chapter" data-level="2.1.5" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#study-concepts"><i class="fa fa-check"></i><b>2.1.5</b> Study Concepts</a></li>
<li class="chapter" data-level="2.1.6" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#randomization-control-and-causation"><i class="fa fa-check"></i><b>2.1.6</b> Randomization, control, and causation</a></li>
<li class="chapter" data-level="2.1.7" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#populations-and-scope-of-inference"><i class="fa fa-check"></i><b>2.1.7</b> Populations and scope of inference</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#data-summaries"><i class="fa fa-check"></i><b>2.2</b> Data Summaries</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#numerical-summaries"><i class="fa fa-check"></i><b>2.2.1</b> Numerical Summaries</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#visual-summaries"><i class="fa fa-check"></i><b>2.2.2</b> Visual Summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-analysis-and-statistical-inference.html"><a href="data-analysis-and-statistical-inference.html#statistical-inference"><i class="fa fa-check"></i><b>2.3</b> Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html"><i class="fa fa-check"></i><b>3</b> Introduction to Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#defining-the-linear-model"><i class="fa fa-check"></i><b>3.1</b> Defining the linear model</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#gauss-markov-model-for-one-sample"><i class="fa fa-check"></i><b>3.2</b> Gauss-Markov model for one sample</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#gauss-markov-model-for-comparing-two-samples"><i class="fa fa-check"></i><b>3.3</b> Gauss-Markov model for comparing two samples</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#pairwise-testing-and-bonferroni-correction"><i class="fa fa-check"></i><b>3.4</b> Pairwise testing and Bonferroni correction</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#example-co2-uptake-in-echinochloa-crus-galli"><i class="fa fa-check"></i><b>3.4.1</b> Example: Co2 uptake in Echinochloa crus-galli</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-linear-models.html"><a href="introduction-to-linear-models.html#linear-models-for-more-than-two-treatments-populations"><i class="fa fa-check"></i><b>3.5</b> Linear Models for more than two treatments (populations)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html"><i class="fa fa-check"></i><b>4</b> Randomization and Permutation Testing</a>
<ul>
<li class="chapter" data-level="4.1" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#setup"><i class="fa fa-check"></i><b>4.1</b> Setup</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#example-1-the-harris-bank-sex-pay-study"><i class="fa fa-check"></i><b>4.1.1</b> Example 1: The Harris Bank Sex Pay Study</a></li>
<li class="chapter" data-level="4.1.2" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#difference-in-mean-salaries-between-genders"><i class="fa fa-check"></i><b>4.1.2</b> Difference in mean salaries between genders</a></li>
<li class="chapter" data-level="4.1.3" data-path="randomization-and-permutation-testing.html"><a href="randomization-and-permutation-testing.html#randomization-test-for-treatment-significance"><i class="fa fa-check"></i><b>4.1.3</b> Randomization test for treatment significance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html"><i class="fa fa-check"></i><b>5</b> Alternatives for Non-Normal Responses</a>
<ul>
<li class="chapter" data-level="5.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#example-cloud-seeding-for-rainfall"><i class="fa fa-check"></i><b>5.1.1</b> Example: Cloud Seeding for Rainfall</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#rank-sum-test"><i class="fa fa-check"></i><b>5.2</b> Rank-Sum Test</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#example-ratio-measurements"><i class="fa fa-check"></i><b>5.2.1</b> Example: ratio measurements</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#signed-rank-test"><i class="fa fa-check"></i><b>5.3</b> Signed-rank test</a></li>
<li class="chapter" data-level="5.4" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrap"><i class="fa fa-check"></i><b>5.4</b> Bootstrap</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#intro-to-the-bootstrap"><i class="fa fa-check"></i><b>5.4.1</b> Intro to the bootstrap</a></li>
<li class="chapter" data-level="5.4.2" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.4.2</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="5.4.3" data-path="alternatives-for-non-normal-responses.html"><a href="alternatives-for-non-normal-responses.html#bootstrapping-linear-models"><i class="fa fa-check"></i><b>5.4.3</b> Bootstrapping linear models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>6</b> One Way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1" data-path="one-way-anova.html"><a href="one-way-anova.html#the-gauss-markov-model-for-comparing-i-populations"><i class="fa fa-check"></i><b>6.1</b> The Gauss-Markov Model for comparing I populations</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="one-way-anova.html"><a href="one-way-anova.html#example-donuts-effects-model"><i class="fa fa-check"></i><b>6.1.1</b> Example: Donuts effects model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="one-way-anova.html"><a href="one-way-anova.html#testing-equality-of-means"><i class="fa fa-check"></i><b>6.2</b> Testing equality of means</a></li>
<li class="chapter" data-level="6.3" data-path="one-way-anova.html"><a href="one-way-anova.html#follow-up-testing"><i class="fa fa-check"></i><b>6.3</b> Follow-up testing</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="one-way-anova.html"><a href="one-way-anova.html#donuts-example"><i class="fa fa-check"></i><b>6.3.1</b> Donuts example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-way-anova.html"><a href="one-way-anova.html#inference-for-particular-contrasts"><i class="fa fa-check"></i><b>6.4</b> Inference for particular contrasts</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#donuts-example-1"><i class="fa fa-check"></i><b>6.4.1</b> Donuts Example</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="one-way-anova.html"><a href="one-way-anova.html#checking-assumptions-in-one-way-anova"><i class="fa fa-check"></i><b>6.5</b> Checking Assumptions in one-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="one-way-anova.html"><a href="one-way-anova.html#example-cehcking-assumptions-for-donuts-experiment"><i class="fa fa-check"></i><b>6.5.1</b> Example: Cehcking assumptions for donuts experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html"><i class="fa fa-check"></i><b>7</b> Randomized Complete Block Design</a>
<ul>
<li class="chapter" data-level="7.1" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#paired-experiments-as-blocking"><i class="fa fa-check"></i><b>7.1</b> Paired experiments as blocking</a></li>
<li class="chapter" data-level="7.2" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#randomized-complete-block-designs"><i class="fa fa-check"></i><b>7.2</b> Randomized Complete Block Designs</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#notation"><i class="fa fa-check"></i><b>7.2.1</b> Notation</a></li>
<li class="chapter" data-level="7.2.2" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-penicillin-manufacturing"><i class="fa fa-check"></i><b>7.2.2</b> Example: Penicillin Manufacturing</a></li>
<li class="chapter" data-level="7.2.3" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#sums-of-squares-and-f-tests-in-rcbd"><i class="fa fa-check"></i><b>7.2.3</b> Sums of squares and F tests in RCBD</a></li>
<li class="chapter" data-level="7.2.4" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-significance-of-process-in-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.4</b> Example: Significance of Process in Penicillin experiment</a></li>
<li class="chapter" data-level="7.2.5" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#follow-up-comparisons-and-contrasts"><i class="fa fa-check"></i><b>7.2.5</b> Follow-up comparisons and contrasts</a></li>
<li class="chapter" data-level="7.2.6" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-tukey-contrasts-and-scheffe-for-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.6</b> Example: Tukey, contrasts, and Scheffe for penicillin experiment</a></li>
<li class="chapter" data-level="7.2.7" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#relative-efficiency-of-blocking-vs.-crd"><i class="fa fa-check"></i><b>7.2.7</b> Relative efficiency of Blocking (vs. CRD)</a></li>
<li class="chapter" data-level="7.2.8" data-path="randomized-complete-block-design.html"><a href="randomized-complete-block-design.html#example-re-of-blocking-in-penicillin-experiment"><i class="fa fa-check"></i><b>7.2.8</b> Example: RE of blocking in penicillin experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html"><i class="fa fa-check"></i><b>8</b> Latin Squares for two blocking variables</a>
<ul>
<li class="chapter" data-level="8.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#designs-for-multiple-blocks"><i class="fa fa-check"></i><b>8.1</b> Designs for multiple blocks</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-mpg-experiment"><i class="fa fa-check"></i><b>8.1.1</b> Example: MPG experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#model-notation"><i class="fa fa-check"></i><b>8.2</b> Model notation</a></li>
<li class="chapter" data-level="8.3" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#sums-of-squares-and-test-for-treatment-effects"><i class="fa fa-check"></i><b>8.3</b> Sums of squares and test for treatment effects</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-mpg-experiment-1"><i class="fa fa-check"></i><b>8.3.1</b> Example: MPG experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#relative-efficiency-of-blocking"><i class="fa fa-check"></i><b>8.4</b> Relative efficiency of blocking</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="latin-squares-for-two-blocking-variables.html"><a href="latin-squares-for-two-blocking-variables.html#example-re-ignoring-driver-blocks"><i class="fa fa-check"></i><b>8.4.1</b> Example: RE ignoring driver blocks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="two-way-anova.html"><a href="two-way-anova.html"><i class="fa fa-check"></i><b>9</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="9.1" data-path="two-way-anova.html"><a href="two-way-anova.html#the-model"><i class="fa fa-check"></i><b>9.1</b> The Model</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="two-way-anova.html"><a href="two-way-anova.html#example-protein-chemistry"><i class="fa fa-check"></i><b>9.1.1</b> Example: protein chemistry</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="two-way-anova.html"><a href="two-way-anova.html#tests-for-interaction-and-main-effects"><i class="fa fa-check"></i><b>9.2</b> Tests for interaction and main effects</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="two-way-anova.html"><a href="two-way-anova.html#example-protein-and-dietary-metals"><i class="fa fa-check"></i><b>9.2.1</b> Example: Protein and dietary metals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html"><i class="fa fa-check"></i><b>10</b> Unbalanced data in Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="10.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#raw-and-least-squares-means"><i class="fa fa-check"></i><b>10.1</b> Raw and Least-Squares Means</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data"><i class="fa fa-check"></i><b>10.1.1</b> Example: Turbine data</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#partial-f-tests-and-type-iii-ss"><i class="fa fa-check"></i><b>10.2</b> Partial F tests and Type III SS</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data-tests"><i class="fa fa-check"></i><b>10.2.1</b> Example: Turbine Data tests</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#follow-up-tests"><i class="fa fa-check"></i><b>10.3</b> Follow-up tests</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#example-turbine-data-tukey-corrected-pairwise-comparisons"><i class="fa fa-check"></i><b>10.3.1</b> Example: Turbine data Tukey-corrected pairwise comparisons</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="unbalanced-data-in-two-way-anova.html"><a href="unbalanced-data-in-two-way-anova.html#unbalanced-data-and-simpsons-paradox"><i class="fa fa-check"></i><b>10.4</b> Unbalanced data and Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html"><i class="fa fa-check"></i><b>11</b> Missing Cells in Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="11.1" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#anova-with-missing-at-random-cell"><i class="fa fa-check"></i><b>11.1</b> ANOVA with missing “at random” cell</a></li>
<li class="chapter" data-level="11.2" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#do-any-of-the-treatments-matter"><i class="fa fa-check"></i><b>11.2</b> Do any of the treatments matter?</a></li>
<li class="chapter" data-level="11.3" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#fit-a-linear-model-with-additional-constraints"><i class="fa fa-check"></i><b>11.3</b> Fit a linear model with additional constraints</a></li>
<li class="chapter" data-level="11.4" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#general-linear-test-for-interaction"><i class="fa fa-check"></i><b>11.4</b> General Linear Test for interaction</a></li>
<li class="chapter" data-level="11.5" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#partial-analysis-for-interaction"><i class="fa fa-check"></i><b>11.5</b> Partial analysis for interaction</a></li>
<li class="chapter" data-level="11.6" data-path="missing-cells-in-two-way-anova.html"><a href="missing-cells-in-two-way-anova.html#tests-for-main-effects-in-the-partial-table"><i class="fa fa-check"></i><b>11.6</b> Tests for main effects in the partial table</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html"><i class="fa fa-check"></i><b>12</b> Strategy for Analysis in Two-Factor models</a>
<ul>
<li class="chapter" data-level="12.1" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#unbalanced-two-factor-experiment-chick-weight"><i class="fa fa-check"></i><b>12.1</b> Unbalanced Two-Factor Experiment: Chick Weight</a></li>
<li class="chapter" data-level="12.2" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#example-observational-study-on-growth-hormone-deficient-children-with-empty-cell"><i class="fa fa-check"></i><b>12.2</b> Example: observational study on growth-hormone deficient children with empty cell</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#linear-model-analysis-with-extra-constraint"><i class="fa fa-check"></i><b>12.2.1</b> Linear Model analysis with extra constraint</a></li>
<li class="chapter" data-level="12.2.2" data-path="strategy-for-analysis-in-two-factor-models.html"><a href="strategy-for-analysis-in-two-factor-models.html#partial-table-analysis"><i class="fa fa-check"></i><b>12.2.2</b> Partial Table analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html"><i class="fa fa-check"></i><b>13</b> <span class="math inline">\(2^k\)</span> Factorial Experiments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#the-model-1"><i class="fa fa-check"></i><b>13.1</b> The Model</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#example-stress-test-study"><i class="fa fa-check"></i><b>13.1.1</b> Example: Stress Test Study</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#unreplicated-factorial-experiments"><i class="fa fa-check"></i><b>13.2</b> Unreplicated Factorial Experiments</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="k-factorial-experiments.html"><a href="k-factorial-experiments.html#example-granola-bar-experiment"><i class="fa fa-check"></i><b>13.2.1</b> Example: Granola bar experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>14</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="14.1" data-path="linear-regression.html"><a href="linear-regression.html#diamonds-dataset"><i class="fa fa-check"></i><b>14.1</b> Diamonds dataset</a></li>
<li class="chapter" data-level="14.2" data-path="linear-regression.html"><a href="linear-regression.html#checking-linearity"><i class="fa fa-check"></i><b>14.2</b> Checking Linearity</a></li>
<li class="chapter" data-level="14.3" data-path="linear-regression.html"><a href="linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>14.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="14.4" data-path="linear-regression.html"><a href="linear-regression.html#model-with-carat-interactions"><i class="fa fa-check"></i><b>14.4</b> Model with carat interactions</a></li>
<li class="chapter" data-level="14.5" data-path="linear-regression.html"><a href="linear-regression.html#interpreting-the-model"><i class="fa fa-check"></i><b>14.5</b> Interpreting the model</a></li>
<li class="chapter" data-level="14.6" data-path="linear-regression.html"><a href="linear-regression.html#checking-constant-variance"><i class="fa fa-check"></i><b>14.6</b> Checking Constant Variance</a></li>
<li class="chapter" data-level="14.7" data-path="linear-regression.html"><a href="linear-regression.html#normality"><i class="fa fa-check"></i><b>14.7</b> Normality</a></li>
<li class="chapter" data-level="14.8" data-path="linear-regression.html"><a href="linear-regression.html#addressing-non-constant-variance-using-wls"><i class="fa fa-check"></i><b>14.8</b> Addressing non-constant variance using WLS</a></li>
<li class="chapter" data-level="14.9" data-path="linear-regression.html"><a href="linear-regression.html#inferences-using-wls"><i class="fa fa-check"></i><b>14.9</b> Inferences using WLS</a></li>
<li class="chapter" data-level="14.10" data-path="linear-regression.html"><a href="linear-regression.html#using-built-in-functions-in-r-for-inference"><i class="fa fa-check"></i><b>14.10</b> Using built-in functions in R for inference</a></li>
<li class="chapter" data-level="14.11" data-path="linear-regression.html"><a href="linear-regression.html#leverage-outliers-and-influence"><i class="fa fa-check"></i><b>14.11</b> Leverage, outliers, and influence</a>
<ul>
<li class="chapter" data-level="14.11.1" data-path="linear-regression.html"><a href="linear-regression.html#illustrations"><i class="fa fa-check"></i><b>14.11.1</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="linear-regression.html"><a href="linear-regression.html#numerical-summaries-of-outliers-leverage-and-influence"><i class="fa fa-check"></i><b>14.12</b> Numerical summaries of outliers, leverage, and influence</a></li>
<li class="chapter" data-level="14.13" data-path="linear-regression.html"><a href="linear-regression.html#leverage-diamonds-model"><i class="fa fa-check"></i><b>14.13</b> Leverage, Diamonds model</a>
<ul>
<li class="chapter" data-level="14.13.1" data-path="linear-regression.html"><a href="linear-regression.html#outliers-diamonds-model"><i class="fa fa-check"></i><b>14.13.1</b> Outliers, diamonds model</a></li>
<li class="chapter" data-level="14.13.2" data-path="linear-regression.html"><a href="linear-regression.html#plotting-outliers-with-leverage"><i class="fa fa-check"></i><b>14.13.2</b> plotting outliers with leverage</a></li>
<li class="chapter" data-level="14.13.3" data-path="linear-regression.html"><a href="linear-regression.html#cooks-distance-df-betas-df-fits"><i class="fa fa-check"></i><b>14.13.3</b> Cook’s distance, DF betas, DF fits</a></li>
<li class="chapter" data-level="14.13.4" data-path="linear-regression.html"><a href="linear-regression.html#model-fit-without-high-leverage-outliers"><i class="fa fa-check"></i><b>14.13.4</b> Model fit without high leverage outliers</a></li>
</ul></li>
<li class="chapter" data-level="14.14" data-path="linear-regression.html"><a href="linear-regression.html#dealing-with-highly-influential-data-points"><i class="fa fa-check"></i><b>14.14</b> Dealing with highly influential data points</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>15</b> Analysis of Covariance</a>
<ul>
<li class="chapter" data-level="15.1" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#choice-of-concommitant-variable"><i class="fa fa-check"></i><b>15.1</b> Choice of “concommitant” variable</a></li>
<li class="chapter" data-level="15.2" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#interaction-and-regression-slopes"><i class="fa fa-check"></i><b>15.2</b> Interaction and regression slopes</a></li>
<li class="chapter" data-level="15.3" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#inference-questions"><i class="fa fa-check"></i><b>15.3</b> Inference questions</a></li>
<li class="chapter" data-level="15.4" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html#example"><i class="fa fa-check"></i><b>15.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html"><i class="fa fa-check"></i><b>16</b> Gauss-Markov Theory</a>
<ul>
<li class="chapter" data-level="16.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#estimability"><i class="fa fa-check"></i><b>16.1</b> Estimability</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-one-way-anova"><i class="fa fa-check"></i><b>16.1.1</b> Characterizing estimable functions in one-way ANOVA</a></li>
<li class="chapter" data-level="16.1.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-using-the-null-space-of-x"><i class="fa fa-check"></i><b>16.1.2</b> Characterizing estimable functions in RCBD using the null space of <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="16.1.3" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-by-solving-an-inhomogeneous-equation"><i class="fa fa-check"></i><b>16.1.3</b> Characterizing estimable functions in RCBD by solving an inhomogeneous equation</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#estimation"><i class="fa fa-check"></i><b>16.2</b> Estimation</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#full-rank-estimation"><i class="fa fa-check"></i><b>16.2.1</b> Full rank estimation</a></li>
<li class="chapter" data-level="16.2.2" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#less-than-full-rank-estimation"><i class="fa fa-check"></i><b>16.2.2</b> Less than full rank estimation</a></li>
<li class="chapter" data-level="16.2.3" data-path="gauss-markov-theory.html"><a href="gauss-markov-theory.html#augmented-normal-equations"><i class="fa fa-check"></i><b>16.2.3</b> Augmented normal equations</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gauss-markov-theory" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">Chapter 16</span> Gauss-Markov Theory<a href="gauss-markov-theory.html#gauss-markov-theory" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Recall the Gauss-Markov model defines a linear, stochastic relationship between an <span class="math inline">\(n\times 1\)</span> vector response <span class="math inline">\(Y\)</span> and an <span class="math inline">\(n\times p\)</span> matrix of covariates <span class="math inline">\(X\)</span> by
<span class="math display">\[Y = X\beta + \epsilon\]</span>
where <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)\)</span>.<br><br></p>
<p>Gauss-Markov theory is concerned with the following questions:<br>
- Which linear functions <span class="math inline">\(c^\top \beta\)</span> are meaningful and may be estimated by the observed <span class="math inline">\((Y,X)\)</span>?
- How should such functions be estimated?
- Are hypotheses about such functions testable using the observed(Y,X)?
- How may such testable hypotheses be tested? (What are the test statistics and their corresponding sampling distributions?)</p>
<div id="estimability" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> Estimability<a href="gauss-markov-theory.html#estimability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In general, beyond Gauss-Markov models, whenever we use a probability distribution to model a population there is a notion of <em>identifiability</em> with respect to the parameter of that model distribution. Say <span class="math inline">\(F_\theta\)</span> is a distribution function used to model a population where <span class="math inline">\(\theta\)</span> is the parameter indexing the family of distributions. Then, we say <span class="math inline">\(\theta\)</span> is identifiable if and only if <span class="math inline">\(\theta\mapsto F_\theta\)</span> one-to-one. In other words, two different parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta&#39;\)</span> do not correspond to the same distribution function. <br><br></p>
<p>In the context of Gauss-Markov models the notion of identifiability boils down to:
<span class="math display">\[\beta\text{ is identifiable if }X\beta_1\ne X\beta_2\iff \beta_1\ne \beta_2.\]</span>
And, from the point of view of linear algebra this means that <span class="math inline">\(\beta\)</span> is estimable if and only if <span class="math inline">\(X\)</span> has full column rank—<span class="math inline">\(Xa = 0 \iff a \equiv 0\)</span>.<br><br></p>
<p>A linear function <span class="math inline">\(c^\top \beta\)</span> is <em>estimable</em> if it can be written
<span class="math display">\[c^\top \beta = AX\beta = AE(Y)\]</span>
for some matrix <span class="math inline">\(A\)</span>. Statistically, we understand estimability to mean that the function in question can be represented as one or more linear combinations of mean responses. From a linear algebra point of view, a linear function <span class="math inline">\(c^\top \beta\)</span> is estimable if <span class="math inline">\(c^\top = AX\)</span> for some <span class="math inline">\(A\)</span>, which means <span class="math inline">\(c\)</span> is in the column space of <span class="math inline">\(X^\top\)</span> (row space of X).</p>
<div id="characterizing-estimable-functions-in-one-way-anova" class="section level3 hasAnchor" number="16.1.1">
<h3><span class="header-section-number">16.1.1</span> Characterizing estimable functions in one-way ANOVA<a href="gauss-markov-theory.html#characterizing-estimable-functions-in-one-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a one-way ANOVA where <span class="math inline">\(n=12\)</span> with 3 groups of 4 replicates:
<span class="math display">\[Y_{ij} = \mu + \alpha_i + \epsilon_{ij}.\]</span>
The design matrix of the equivalent Gauss-Markov model <span class="math inline">\(Y = X\beta+\epsilon\)</span> is
<span class="math display">\[X = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix},\]</span>
and the coefficient vector is <span class="math inline">\(\beta = (\mu,\alpha_1, \alpha_2, \alpha_3)^\top\)</span>. <br><br>
The coefficient vector <span class="math inline">\(\beta\)</span> is <strong>NOT</strong> identifiable or estimable. To see this, first define <span class="math inline">\(\mu_i\)</span> <span class="math inline">\(i=1,2,3\)</span> to be the population means of the three groups. Then, let <span class="math inline">\(\beta_1 = (0,\mu_1, \mu_2, \mu_3)^\top\)</span> and <span class="math inline">\(\beta_2 = (\mu_3, \mu_1 - \mu_3, \mu_2 = \mu_3, 0)^\top\)</span>. Then, <span class="math inline">\(\beta_1 \ne \beta_2\)</span>, generally; in fact, <span class="math inline">\(\beta_1 - \beta_2 = (-\mu_3, \mu_3, \mu_3, \mu_3)\)</span>. But, it is easy to check that <span class="math inline">\(X\beta_1 = X\beta_2 = 4\mu_1+4\mu_2+4\mu_3\)</span>.</p>
<p><br><br></p>
<p>The parameter <span class="math inline">\(\mu + \alpha_1\)</span>, however, IS estimable. To see this, first write
<span class="math display">\[\mu+\alpha_1 = c^\top \beta = (1,1,0,0)\cdot (\mu, \alpha_1, \alpha_2, \alpha_3)^\top.\]</span>
Let <span class="math inline">\(A = (\tfrac14, \tfrac14, \tfrac14, \tfrac14, 0,0,0,0,0,0,0,0)\)</span> and note that
<span class="math display">\[E(Y) = X\beta = (\mu+\alpha_1, \mu+\alpha_1, \mu+\alpha_1, \mu+\alpha_1, \mu+\alpha_2,\cdots, \mu+\alpha_4).\]</span>
Therefore, <span class="math inline">\(AE(Y) = \tfrac14\sum_{j=1}^4 E(Y_{1j}) = \mu+\alpha_1\)</span>. Since we were able to find a linear combination of mean responses equal to the parameter that means the function <span class="math inline">\(c^\top \beta\)</span> is estimable.
<br><br></p>
<p>Last, let’s think carefully about how to characterize estimable and non-estimable functions more generally, rather than using ad-hoc methods in consideration of individual functions like those above. Recall that the definition of estimability says that <span class="math inline">\(c\)</span> is in the row space of <span class="math inline">\(X\)</span>, i.e., it is equivalent to a linear combination of rows of <span class="math inline">\(X\)</span>. That means <span class="math inline">\(X^\top y = c\)</span> for some <span class="math inline">\(y\)</span>. If there is a solution <span class="math inline">\(y\)</span> to this inhomogeneous system then <span class="math inline">\(c^\top \beta\)</span> is estimable. One way to check this condition is to investigate the null space of <span class="math inline">\(X\)</span>, which is the set of vectors <span class="math inline">\(u\)</span> such that <span class="math inline">\(Xu = 0\)</span>. If <span class="math inline">\(c\)</span> is orthogonal to any set of basis vectors spanning the null space of <span class="math inline">\(X\)</span>—denoted <span class="math inline">\(N(X)\)</span>—then <span class="math inline">\(c\)</span> is in the row space of <span class="math inline">\(X\)</span>, hence <span class="math inline">\(c^\top \beta\)</span> is estimable.<br><br></p>
<p>Let’s apply this strategy to the one-way ANOVA example above. By inspection, we can tell that only three of the four columns in <span class="math inline">\(X\)</span> are linearly independent, for example, because the first column equals the sum of the other three. Another way to confirm this is to produce the reduced row echelon form (RREF) of <span class="math inline">\(X\)</span> via Gaussian elimination; there are only three non-zero rows in the RREF of <span class="math inline">\(X\)</span>, hence the rank of <span class="math inline">\(X\)</span> is 3. (See the R code below). Because the dimension of the row space and null space of <span class="math inline">\(X\)</span> must add to the number of columns of <span class="math inline">\(X\)</span> (the Rank + Nullity Theorem) it follows that the null space of <span class="math inline">\(X\)</span> is one-dimensional. In other words, the null space is spanned by a single vector. Using the RREF we can see that a solution to <span class="math inline">\(Xu = 0\)</span> is <span class="math inline">\(u = (1,-1,-1,-1)^\top\)</span>. Every function <span class="math inline">\(c^\top \beta\)</span> such that the rows of <span class="math inline">\(c^\top\)</span> are orthogonal to <span class="math inline">\(u\)</span> is estimable. For example, <span class="math inline">\((1,1,0,0)\cdot u = 0\)</span>; hence, <span class="math inline">\(\mu+\alpha_1\)</span> is estimable.</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="gauss-markov-theory.html#cb691-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(matlib)</span>
<span id="cb691-2"><a href="gauss-markov-theory.html#cb691-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">12</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb691-3"><a href="gauss-markov-theory.html#cb691-3" aria-hidden="true" tabindex="-1"></a><span class="fu">echelon</span>(X)</span></code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4]
##  [1,]    1    0    0    1
##  [2,]    0    1    0   -1
##  [3,]    0    0    1   -1
##  [4,]    0    0    0    0
##  [5,]    0    0    0    0
##  [6,]    0    0    0    0
##  [7,]    0    0    0    0
##  [8,]    0    0    0    0
##  [9,]    0    0    0    0
## [10,]    0    0    0    0
## [11,]    0    0    0    0
## [12,]    0    0    0    0</code></pre>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="gauss-markov-theory.html#cb693-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(X, <span class="fu">matrix</span>(<span class="dv">0</span>,<span class="dv">12</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1       + x4  =  0 
##   x2   - 1*x4  =  0 
##     x3 - 1*x4  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0 
##             0  =  0</code></pre>
</div>
<div id="characterizing-estimable-functions-in-rcbd-using-the-null-space-of-x" class="section level3 hasAnchor" number="16.1.2">
<h3><span class="header-section-number">16.1.2</span> Characterizing estimable functions in RCBD using the null space of <span class="math inline">\(X\)</span><a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-using-the-null-space-of-x" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A linear model for an RCBD is
<span class="math display">\[Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}.\]</span>
Suppose, for example, that there are 12 observations, 4 in each of 3 blocks, each with a different treatment. Then, the design matrix may be written
<span class="math display">\[X = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1
\end{bmatrix}.\]</span>
The rank of <span class="math inline">\(X\)</span> is 5, which can be determined by solving the system <span class="math inline">\(Xu = 0\)</span> by Gaussian elimination and observing that the RREF of <span class="math inline">\(X\)</span> contains 5 non-zero rows. The rank plus nullity theorem implies the null space is two-dimensional. Two linearly independent solutions to the homogeneous system are
<span class="math display">\[u = (-2,1,1,1,1,1,1)^\top \text{ and } v = (-1,1,1,1,1,0,0)^\top\]</span>
which together form a basis for <span class="math inline">\(N(X)\)</span>, the null space of <span class="math inline">\(X\)</span>. All estimable linear functions <span class="math inline">\(c^\top \beta\)</span> are such that the rows of <span class="math inline">\(c^\top\)</span> are orthogonal to both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, and hence orthogonal to the null space, or, in other words, members of the row space of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(R(X)\)</span>. <br><br></p>
<p>For examples, <span class="math inline">\(\alpha_1\)</span> is not estimable, nor is <span class="math inline">\(\mu + \alpha_1 - \alpha_2\)</span>. But, <span class="math inline">\(\mu + \alpha_1 + \tfrac12(\beta_1 + \beta_2)\)</span> is estimable. You can check that the corresponding <span class="math inline">\(c\)</span> vectors for these functions are <span class="math inline">\((0,1,0,0,0,0,0)\)</span>, <span class="math inline">\((1,1,-1,0,0,0,0)\)</span> and <span class="math inline">\((1,1,0,0,0,\tfrac12,\tfrac12)\)</span>, of which only the last is orthogonal to the null space basis found above.
<br><br></p>
<p>The codes below illustrate how to determine the RREF and rank of the design matrix <span class="math inline">\(X\)</span> using the package matlib in R.</p>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="gauss-markov-theory.html#cb695-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(matlib)</span>
<span id="cb695-2"><a href="gauss-markov-theory.html#cb695-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,</span>
<span id="cb695-3"><a href="gauss-markov-theory.html#cb695-3" aria-hidden="true" tabindex="-1"></a>          <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb695-4"><a href="gauss-markov-theory.html#cb695-4" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb695-5"><a href="gauss-markov-theory.html#cb695-5" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb695-6"><a href="gauss-markov-theory.html#cb695-6" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,</span>
<span id="cb695-7"><a href="gauss-markov-theory.html#cb695-7" aria-hidden="true" tabindex="-1"></a>          <span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,</span>
<span id="cb695-8"><a href="gauss-markov-theory.html#cb695-8" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="dv">8</span>,<span class="dv">7</span>)</span>
<span id="cb695-9"><a href="gauss-markov-theory.html#cb695-9" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    1    1    0    0    0    1    0
## [2,]    1    1    0    0    0    0    1
## [3,]    1    0    1    0    0    1    0
## [4,]    1    0    1    0    0    0    1
## [5,]    1    0    0    1    0    1    0
## [6,]    1    0    0    1    0    0    1
## [7,]    1    0    0    0    1    1    0
## [8,]    1    0    0    0    1    0    1</code></pre>
<p>The basis vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are determined by putting x6=x7=1 and x6=x7=0 in the general solution of the homogeneous system produced by the Solve function.</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="gauss-markov-theory.html#cb697-1" aria-hidden="true" tabindex="-1"></a><span class="fu">echelon</span>(X)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    1    0    0    0    1    0    1
## [2,]    0    1    0    0   -1    0    0
## [3,]    0    0    1    0   -1    0    0
## [4,]    0    0    0    1   -1    0    0
## [5,]    0    0    0    0    0    1   -1
## [6,]    0    0    0    0    0    0    0
## [7,]    0    0    0    0    0    0    0
## [8,]    0    0    0    0    0    0    0</code></pre>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="gauss-markov-theory.html#cb699-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(X,<span class="fu">matrix</span>(<span class="dv">0</span>,<span class="dv">8</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1         + x5     + x7  =  0 
##   x2     - 1*x5           =  0 
##     x3   - 1*x5           =  0 
##       x4 - 1*x5           =  0 
##                x6 - 1*x7  =  0 
##                        0  =  0 
##                        0  =  0 
##                        0  =  0</code></pre>
<p>By solving <span class="math inline">\(Ax = 0\)</span> where A is the null space basis of <span class="math inline">\(X\)</span> we find that an estimable linear functions <span class="math inline">\(c^\top \beta\)</span> is such that each row of <span class="math inline">\(c^\top\)</span> satisfies <span class="math inline">\(c_1 = c_6+c_7\)</span> and <span class="math inline">\(c_2+c_3+c_4+c_5 = c_6+c_7\)</span>.</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="gauss-markov-theory.html#cb701-1" aria-hidden="true" tabindex="-1"></a>null.basis <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">7</span>,<span class="dv">2</span>))</span>
<span id="cb701-2"><a href="gauss-markov-theory.html#cb701-2" aria-hidden="true" tabindex="-1"></a>null.basis</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]   -2    1    1    1    1    1    1
## [2,]   -1    1    1    1    1    0    0</code></pre>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="gauss-markov-theory.html#cb703-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(null.basis, <span class="fu">matrix</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1                  - 1*x6 - 1*x7  =  0 
##   x2 + x3 + x4 + x5 - 1*x6 - 1*x7  =  0</code></pre>
</div>
<div id="characterizing-estimable-functions-in-rcbd-by-solving-an-inhomogeneous-equation" class="section level3 hasAnchor" number="16.1.3">
<h3><span class="header-section-number">16.1.3</span> Characterizing estimable functions in RCBD by solving an inhomogeneous equation<a href="gauss-markov-theory.html#characterizing-estimable-functions-in-rcbd-by-solving-an-inhomogeneous-equation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall once more that the definition of estimability says the linear function <span class="math inline">\(c^\top \beta\)</span> is estimable if and only if <span class="math inline">\(c^\top \beta = AX\beta\)</span> which means that <span class="math inline">\(c^\top = AX\)</span> or, rather that <span class="math inline">\(X^\top A^\top = c\)</span>. Now, <span class="math inline">\(X^\top\)</span> is a <span class="math inline">\(p\times n\)</span> matrix and <span class="math inline">\(c\)</span> is a <span class="math inline">\(p \times k\)</span> matrix so that <span class="math inline">\(A^\top\)</span> is a <span class="math inline">\(n\times k\)</span> matrix. From a linear algebraic point of view, estimability means that there exists a solution <span class="math inline">\(a_\ell\)</span> to <span class="math inline">\(X^\top a_\ell = c_\ell\)</span> for all <span class="math inline">\(\ell = 1, \ldots, k\)</span> where <span class="math inline">\(a_\ell\)</span> and <span class="math inline">\(c_\ell\)</span> denote the columns of <span class="math inline">\(A^\top\)</span> and <span class="math inline">\(c\)</span>, respectively. Inhomogeneous systems may be solved by Gauss-Jordan elimination on the augmented matrix <span class="math inline">\([X^\top ,\,c]\)</span>; and see the R codes below.<br><br>
The first inhomogeneous system below is used to evaluate whether <span class="math inline">\(\mu + \alpha_1 - \alpha_2\)</span> is estimable, corresponding to <span class="math inline">\(c = (1,1,-1,0,0,0,0)\)</span>. It is not estimable, and this is reflected in the inconsistencies “0=1” in the RREF of the augmented matrix <span class="math inline">\([X, \, c]\)</span>. On the other hand, <span class="math inline">\(\mu + \alpha_1 + \tfrac12(\beta_1 + \beta_2)\)</span> is estimable, as evidenced by the solutions given to its corresponding inhomogeneous system below. For example, <span class="math inline">\(A = (0.5,0.5,0,0,0,0,0,0)\)</span> is one solution (obtained by putting x8=x7=x6=x5=x4=x3=0), but there are infinite others.</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="gauss-markov-theory.html#cb705-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(<span class="fu">t</span>(X),<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">7</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1     - 1*x4   - 1*x6   - 1*x8  =   1 
##   x2     + x4     + x6     + x8  =   1 
##     x3   + x4                    =  -1 
##              x5   + x6           =   0 
##                       x7   + x8  =   0 
##                               0  =  -1 
##                               0  =  -1</code></pre>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="gauss-markov-theory.html#cb707-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(<span class="fu">t</span>(X),<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>,.<span class="dv">5</span>),<span class="dv">7</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1     - 1*x4   - 1*x6   - 1*x8  =  0.5 
##   x2     + x4     + x6     + x8  =  0.5 
##     x3   + x4                    =    0 
##              x5   + x6           =    0 
##                       x7   + x8  =    0 
##                               0  =    0 
##                               0  =    0</code></pre>
</div>
</div>
<div id="estimation" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> Estimation<a href="gauss-markov-theory.html#estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="full-rank-estimation" class="section level3 hasAnchor" number="16.2.1">
<h3><span class="header-section-number">16.2.1</span> Full rank estimation<a href="gauss-markov-theory.html#full-rank-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far we have discussed <em>whether or not</em> a linear function <span class="math inline">\(c^\top \beta\)</span> may be estimated but not <em>how</em> to estimate it when possible. <br><br></p>
<p>Consider a linear algebraic point of view (forget about statistics momentarily) and recall that if <span class="math inline">\(\beta\)</span> is estimable that means the <span class="math inline">\(p-\)</span>dimensional identity matrix is in the row space of <span class="math inline">\(X\)</span>, and, equivalently, that <span class="math inline">\(X\)</span> has full rank <span class="math inline">\(p\)</span>. Given <span class="math inline">\(\hat\beta\)</span> is an estimator of <span class="math inline">\(\beta\)</span> (we have not yet defined <span class="math inline">\(\hat\beta\)</span>) then <span class="math inline">\(X\hat\beta\)</span> is the natural linear predictor of <span class="math inline">\(Y\)</span>. I say it is the natural predictor because <span class="math inline">\(E(Y) = X\beta\)</span>, meaning that <span class="math inline">\(E(Y)\)</span> is in the columns space of <span class="math inline">\(X\)</span>, and it is natural to predict <span class="math inline">\(Y\)</span> by an estimate of its mean. Therefore, a natural predictor of <span class="math inline">\(Y\)</span> is a vector in the columns space of <span class="math inline">\(X\)</span>. But, which one? Again, proceeding logically, the natural choice is the vector in the columns space of <span class="math inline">\(X\)</span> that is <em>closest</em> to the observed <span class="math inline">\(Y\)</span> with respect to some measure of distance. If we choose to measure distance according to Euclidean (also called <span class="math inline">\(L_2\)</span> distance) then the resulting estimator <span class="math inline">\(\hat\beta\)</span> turns out to be the familiar OLS/least squares estimator (and the MLE).<br><br></p>
<p>To see this, note that the Euclidean distance between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\hat\beta\)</span> is given by
<span class="math display">\[\|Y - X\hat\beta\|_2 = \left(\sum_{i=1}^n (Y_i - x_i^\top \hat\beta)^2\right)^{1/2} = (Y - X\hat\beta)^\top (Y - X\hat\beta)\]</span>
In order to minimize this quantity over <span class="math inline">\(\hat\beta\)</span> we differentiate w.r.t. <span class="math inline">\(\hat\beta\)</span>, set the gradient equal to zero and solve, which produces the <em>normal equations</em>
<span class="math display">\[X^\top X\hat\beta = X^\top Y.\]</span>
Since <span class="math inline">\(X\)</span> is full rank, so is <span class="math inline">\(X^\top X\)</span>, and, therefore, <span class="math inline">\(X^\top X\)</span> being a square matrix is also invertible with inverse <span class="math inline">\((X^\top X)^{-1}\)</span>. Left multiplying the normal equations by the inverse produces the OLS estimator
<span class="math display">\[\hat\beta = (X^\top X)^{-1}X^\top Y.\]</span>
Further, the predictor <span class="math inline">\(\hat Y\)</span> is <span class="math inline">\(X\hat\beta = X(X^\top X)^{-1}X^\top Y = P_x Y\)</span> where <span class="math inline">\(P_x\)</span> is the X-projection matrix, which <em>projects</em> <span class="math inline">\(n-\)</span>vectors to a p-dimensional linear subspace, that is, <span class="math inline">\(\mathbb{R}^p\)</span>, the row space of <span class="math inline">\(X\)</span>.<br><br></p>
<p>Tying back to the definition of estimability, note that it is straightforward to verify any linear function <span class="math inline">\(c^\top \beta\)</span> is estimable when <span class="math inline">\(\hat\beta\)</span> itself is estimable. First, note an estimator is given by <span class="math inline">\(c^\top \hat\beta = c^\top (X^\top X)^{-1}X^\top Y\)</span>. Next, recall that <span class="math inline">\(\hat\beta\)</span> is unbiased, <span class="math inline">\(E(\hat\beta) = E((X^\top X)^{-1}X^\top Y) = (X^\top X)^{-1}X^\top X\beta = \beta\)</span>. Therefore,
<span class="math display">\[E(c^\top \hat\beta) = c^\top \beta = c^\top (X^\top X)^{-1}X^\top E(Y) \]</span>
which reveals that
<span class="math display">\[c^\top \beta = AE(Y)\]</span>
where <span class="math inline">\(A = c^\top (X^\top X)^{-1}X^\top\)</span>; hence, the definition of estimability is satisfied.</p>
</div>
<div id="less-than-full-rank-estimation" class="section level3 hasAnchor" number="16.2.2">
<h3><span class="header-section-number">16.2.2</span> Less than full rank estimation<a href="gauss-markov-theory.html#less-than-full-rank-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When <span class="math inline">\(X\)</span> has rank <span class="math inline">\(r &lt; p\)</span> so does <span class="math inline">\(X^\top X\)</span>, and, as a result, the normal equations <span class="math inline">\(X^\top X \beta = X^\top Y\)</span> have infinite solutions. This means <span class="math inline">\(\beta\)</span> is not estimable. However, for any two solutions to the normal equations, say <span class="math inline">\(\hat\beta\)</span> and <span class="math inline">\(\tilde \beta\)</span>, their predictions are the same:
<span class="math display">\[X\hat\beta = X\tilde \beta.\]</span>
In other words, <span class="math inline">\(\hat Y\)</span> is <em>invariant</em> to the choice of estimate of <span class="math inline">\(\beta\)</span> so long as it is a solution to the normal equations. For a brief explanation, note that if <span class="math inline">\(\hat\beta\)</span> and <span class="math inline">\(\tilde \beta\)</span> both solve the normal equations, then <span class="math inline">\(X^\top X \hat\beta=X^\top X \tilde\beta\)</span>. Additionally, the null space of <span class="math inline">\(X^\top X\)</span> is the same as the null space of <span class="math inline">\(X\)</span>, as can be seen by the following calculation:
<span class="math display">\[\begin{align*}
&amp;X\beta = 0 \Rightarrow X^\top X\beta = 0 \Rightarrow N(X)\subseteq N(X^\top X)\\
&amp; X^\top X\beta = 0 \Rightarrow \beta^\top X^\top X\beta = 0 \Rightarrow (X\beta)^\top (X\beta)=0 \Rightarrow X\beta = 0 \Rightarrow N(X^\top X)\subseteq N(X).
\end{align*}\]</span>
Therefore, <span class="math inline">\(X^\top X (\hat\beta - \tilde \beta) = 0\)</span> implies <span class="math inline">\(X\hat\beta = X\tilde \beta\)</span>.<br />
<br><br>
We have seen that predictions are invariant to the choice of estimator (choice of solution to the normal equations). This means also that functions of the predictions are invariant to the choice of estimator: including the SSE and MSE, and all partial F tests! <br><br></p>
</div>
<div id="augmented-normal-equations" class="section level3 hasAnchor" number="16.2.3">
<h3><span class="header-section-number">16.2.3</span> Augmented normal equations<a href="gauss-markov-theory.html#augmented-normal-equations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Invariance is a very important property. But, given there are infinite solutions to the normal equations in the less-than-full-rank case, how do we pick a solution to use in a statistical analysis? There is a systematic way of doing so, and this method is related to the systems of constraints we considered earlier in the course.<br><br></p>
<p>Let the <span class="math inline">\(n\times p\)</span> design matrix <span class="math inline">\(X\)</span> have rank <span class="math inline">\(r&lt;p\)</span> so that <span class="math inline">\(X^\top X\)</span> also has rank <span class="math inline">\(r\)</span>. Let <span class="math inline">\(K\)</span> specify a <span class="math inline">\((p-r)\times p\)</span> matrix of jointly non-estimable functions, i.e. every linear combination in <span class="math inline">\(K\beta\)</span> is not estimable. <span class="math inline">\(K\)</span> may be found by examining the null space of <span class="math inline">\(X\)</span>: each row vector in <span class="math inline">\(K\)</span> is not-orthogonal to every basis vector in a basis for <span class="math inline">\(N(X)\)</span>. The point is that <span class="math inline">\(K\)</span> is making up for the (<span class="math inline">\(p-r\)</span> dimensional space of) functions that are not estimable due to the lack of full rank of the model matrix. We then augment <span class="math inline">\(X\top X\)</span> by <span class="math inline">\(K\)</span> by row-binding, or stacking, <span class="math inline">\(X^\top X\)</span> on top of <span class="math inline">\(K\)</span> and solve the <em>augmented normal equations</em>
<span class="math display">\[\begin{bmatrix} X^\top X \\ K\end{bmatrix} \beta = \begin{bmatrix} X^\top Y \\0\end{bmatrix}.\]</span>
The augmented equations specify a set of <span class="math inline">\(p-r\)</span> constraints <span class="math inline">\(K\beta = 0\)</span> which force a unique solution to the inhomogeneous system. <br><br></p>
<p>Below is an example of solving the augmented normal equations in the context of one-way ANOVA. Note that the specific choice of augmented rows corresponds to a baseline constraint.</p>
<div class="sourceCode" id="cb709"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb709-1"><a href="gauss-markov-theory.html#cb709-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(matlib)</span>
<span id="cb709-2"><a href="gauss-markov-theory.html#cb709-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The design matrix X</span></span>
<span id="cb709-3"><a href="gauss-markov-theory.html#cb709-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">12</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb709-4"><a href="gauss-markov-theory.html#cb709-4" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4]
##  [1,]    1    1    0    0
##  [2,]    1    1    0    0
##  [3,]    1    1    0    0
##  [4,]    1    1    0    0
##  [5,]    1    0    1    0
##  [6,]    1    0    1    0
##  [7,]    1    0    1    0
##  [8,]    1    0    1    0
##  [9,]    1    0    0    1
## [10,]    1    0    0    1
## [11,]    1    0    0    1
## [12,]    1    0    0    1</code></pre>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="gauss-markov-theory.html#cb711-1" aria-hidden="true" tabindex="-1"></a><span class="co"># X&#39;X</span></span>
<span id="cb711-2"><a href="gauss-markov-theory.html#cb711-2" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(X)<span class="sc">%*%</span>X</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   12    4    4    4
## [2,]    4    4    0    0
## [3,]    4    0    4    0
## [4,]    4    0    0    4</code></pre>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="gauss-markov-theory.html#cb713-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X, <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">4</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1       + x4  =  0 
##   x2   - 1*x4  =  0 
##     x3 - 1*x4  =  0 
##             0  =  0</code></pre>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="gauss-markov-theory.html#cb715-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A basis for the null space is (-1,1,1,1)</span></span>
<span id="cb715-2"><a href="gauss-markov-theory.html#cb715-2" aria-hidden="true" tabindex="-1"></a><span class="co"># A vector not orthogonal to this basis vector is (0,0,0,1)</span></span>
<span id="cb715-3"><a href="gauss-markov-theory.html#cb715-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Augmented Matrix W</span></span>
<span id="cb715-4"><a href="gauss-markov-theory.html#cb715-4" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb715-5"><a href="gauss-markov-theory.html#cb715-5" aria-hidden="true" tabindex="-1"></a>W</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   12    4    4    4
## [2,]    4    4    0    0
## [3,]    4    0    4    0
## [4,]    4    0    0    4
## [5,]    0    0    0    1</code></pre>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="gauss-markov-theory.html#cb717-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some response vector for illustration</span></span>
<span id="cb717-2"><a href="gauss-markov-theory.html#cb717-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">12</span>,X<span class="sc">%*%</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),<span class="dv">4</span>,<span class="dv">1</span>),<span class="dv">1</span>),<span class="dv">12</span>,<span class="dv">1</span>)</span>
<span id="cb717-3"><a href="gauss-markov-theory.html#cb717-3" aria-hidden="true" tabindex="-1"></a>Y</span></code></pre></div>
<pre><code>##           [,1]
##  [1,] 1.787604
##  [2,] 1.597521
##  [3,] 2.171130
##  [4,] 2.721818
##  [5,] 2.024005
##  [6,] 3.417575
##  [7,] 3.248509
##  [8,] 2.091494
##  [9,] 3.930225
## [10,] 5.342587
## [11,] 4.442435
## [12,] 4.615341</code></pre>
<div class="sourceCode" id="cb719"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb719-1"><a href="gauss-markov-theory.html#cb719-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Denote [X&#39;Y 0]&#39; by b</span></span>
<span id="cb719-2"><a href="gauss-markov-theory.html#cb719-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>Y,<span class="dv">0</span>)</span>
<span id="cb719-3"><a href="gauss-markov-theory.html#cb719-3" aria-hidden="true" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>##           [,1]
## [1,] 37.390245
## [2,]  8.278074
## [3,] 10.781582
## [4,] 18.330589
## [5,]  0.000000</code></pre>
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb721-1"><a href="gauss-markov-theory.html#cb721-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the augmented normal equations [X&#39;X K]&#39; beta = [X&#39;Y 0]&#39;,</span></span>
<span id="cb721-2"><a href="gauss-markov-theory.html#cb721-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and note that there is a unique solution and that beta_4 = 0</span></span>
<span id="cb721-3"><a href="gauss-markov-theory.html#cb721-3" aria-hidden="true" tabindex="-1"></a>beta.hat <span class="ot">&lt;-</span> <span class="fu">Solve</span>(W, b)</span></code></pre></div>
<pre><code>## x1        =   4.58264714 
##   x2      =  -2.51312865 
##     x3    =  -1.88725156 
##       x4  =            0 
##        0  =            0</code></pre>
<p>As we have previously shown, <span class="math inline">\(K\)</span> and <span class="math inline">\(K^\top K\)</span> map to the same null space (we showed this w.r.t. the design matrix <span class="math inline">\(X\)</span>). Therefore, an equivalent set of augmented normal equations is given by</p>
<p><span class="math display">\[\begin{bmatrix} X^\top X \\ K^\top K\end{bmatrix} \beta =\begin{bmatrix} X \\ K\end{bmatrix}^\top \begin{bmatrix} X\\ K\end{bmatrix}\beta =\begin{bmatrix} X^\top Y \\0\end{bmatrix}.\]</span></p>
<p>Since <span class="math inline">\([X\,\,\, K]^\top\)</span> is full rank, the product is full rank and <span class="math inline">\((X^\top X + K^\top K)\)</span> has an inverse. We can alternatively write the augmented normal equations as
<span class="math display">\[(X^\top X + K^\top K)\beta = X^\top Y,\]</span>
and we may estimate <span class="math inline">\(\beta\)</span> by <em>constrained least squares</em>:
<span class="math display">\[\hat\beta = (X^\top X + K^\top K)^{-1}X^\top Y\]</span></p>
<p>Next we use this constrained least squares approach in the context of an RCBD example.</p>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="gauss-markov-theory.html#cb723-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(matlib)</span>
<span id="cb723-2"><a href="gauss-markov-theory.html#cb723-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The design matrix X</span></span>
<span id="cb723-3"><a href="gauss-markov-theory.html#cb723-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,</span>
<span id="cb723-4"><a href="gauss-markov-theory.html#cb723-4" aria-hidden="true" tabindex="-1"></a>          <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb723-5"><a href="gauss-markov-theory.html#cb723-5" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb723-6"><a href="gauss-markov-theory.html#cb723-6" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb723-7"><a href="gauss-markov-theory.html#cb723-7" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,</span>
<span id="cb723-8"><a href="gauss-markov-theory.html#cb723-8" aria-hidden="true" tabindex="-1"></a>          <span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,</span>
<span id="cb723-9"><a href="gauss-markov-theory.html#cb723-9" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="dv">8</span>,<span class="dv">7</span>)</span>
<span id="cb723-10"><a href="gauss-markov-theory.html#cb723-10" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    1    1    0    0    0    1    0
## [2,]    1    1    0    0    0    0    1
## [3,]    1    0    1    0    0    1    0
## [4,]    1    0    1    0    0    0    1
## [5,]    1    0    0    1    0    1    0
## [6,]    1    0    0    1    0    0    1
## [7,]    1    0    0    0    1    1    0
## [8,]    1    0    0    0    1    0    1</code></pre>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="gauss-markov-theory.html#cb725-1" aria-hidden="true" tabindex="-1"></a><span class="co"># X&#39;X</span></span>
<span id="cb725-2"><a href="gauss-markov-theory.html#cb725-2" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(X)<span class="sc">%*%</span>X</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    8    2    2    2    2    4    4
## [2,]    2    2    0    0    0    1    1
## [3,]    2    0    2    0    0    1    1
## [4,]    2    0    0    2    0    1    1
## [5,]    2    0    0    0    2    1    1
## [6,]    4    1    1    1    1    4    0
## [7,]    4    1    1    1    1    0    4</code></pre>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb727-1"><a href="gauss-markov-theory.html#cb727-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A basis for the two dimensional null space is (-2,1,1,1,1,1,1) and (-1,1,1,1,1,0,0)</span></span>
<span id="cb727-2"><a href="gauss-markov-theory.html#cb727-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectors not simultaneously orthogonal to these include the baseline constraint vectors (0,0,0,0,1,0,0) and (0,0,0,0,0,0,1)</span></span>
<span id="cb727-3"><a href="gauss-markov-theory.html#cb727-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X, <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dv">7</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## x1         + x5     + x7  =  0 
##   x2     - 1*x5           =  0 
##     x3   - 1*x5           =  0 
##       x4 - 1*x5           =  0 
##                x6 - 1*x7  =  0 
##                        0  =  0 
##                        0  =  0</code></pre>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="gauss-markov-theory.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Augmented Matrix W</span></span>
<span id="cb729-2"><a href="gauss-markov-theory.html#cb729-2" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb729-3"><a href="gauss-markov-theory.html#cb729-3" aria-hidden="true" tabindex="-1"></a>W</span></code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7]
##  [1,]    8    2    2    2    2    4    4
##  [2,]    2    2    0    0    0    1    1
##  [3,]    2    0    2    0    0    1    1
##  [4,]    2    0    0    2    0    1    1
##  [5,]    2    0    0    0    2    1    1
##  [6,]    4    1    1    1    1    4    0
##  [7,]    4    1    1    1    1    0    4
##  [8,]    0    0    0    0    1    0    0
##  [9,]    0    0    0    0    0    0    1</code></pre>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="gauss-markov-theory.html#cb731-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some response vector for illustration</span></span>
<span id="cb731-2"><a href="gauss-markov-theory.html#cb731-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">8</span>,X<span class="sc">%*%</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>),<span class="dv">7</span>,<span class="dv">1</span>),<span class="dv">1</span>),<span class="dv">8</span>,<span class="dv">1</span>)</span>
<span id="cb731-3"><a href="gauss-markov-theory.html#cb731-3" aria-hidden="true" tabindex="-1"></a>Y</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 2.233328
## [2,] 4.286161
## [3,] 4.173176
## [4,] 5.260573
## [5,] 5.449330
## [6,] 5.488295
## [7,] 3.063687
## [8,] 3.984698</code></pre>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="gauss-markov-theory.html#cb733-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Denote [X&#39;Y 0]&#39; by b</span></span>
<span id="cb733-2"><a href="gauss-markov-theory.html#cb733-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>Y,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb733-3"><a href="gauss-markov-theory.html#cb733-3" aria-hidden="true" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>##            [,1]
##  [1,] 33.939247
##  [2,]  6.519489
##  [3,]  9.433749
##  [4,] 10.937625
##  [5,]  7.048384
##  [6,] 14.919520
##  [7,] 19.019727
##  [8,]  0.000000
##  [9,]  0.000000</code></pre>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="gauss-markov-theory.html#cb735-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the augmented normal equations [X&#39;X K]&#39; beta = [X&#39;Y 0]&#39;,</span></span>
<span id="cb735-2"><a href="gauss-markov-theory.html#cb735-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and note that there is a unique solution that zeroes out one treatment and one block coefficient</span></span>
<span id="cb735-3"><a href="gauss-markov-theory.html#cb735-3" aria-hidden="true" tabindex="-1"></a>beta.hat <span class="ot">&lt;-</span> <span class="fu">Solve</span>(W, b)</span></code></pre></div>
<pre><code>## x1              =   4.03671803 
##   x2            =   -0.2644476 
##     x3          =   1.19268231 
##       x4        =   1.94462027 
##         x5      =            0 
##           x6    =  -1.02505168 
##             x7  =            0 
##              0  =            0 
##              0  =            0</code></pre>
<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb737-1"><a href="gauss-markov-theory.html#cb737-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternatively, compute the constrained LS estimator and check it is the same as the solution to the augmented normal equations</span></span>
<span id="cb737-2"><a href="gauss-markov-theory.html#cb737-2" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span>  <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb737-3"><a href="gauss-markov-theory.html#cb737-3" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">t</span>(X)<span class="sc">%*%</span>X <span class="sc">+</span> <span class="fu">t</span>(K)<span class="sc">%*%</span>K</span>
<span id="cb737-4"><a href="gauss-markov-theory.html#cb737-4" aria-hidden="true" tabindex="-1"></a><span class="fu">inv</span>(Z)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span></code></pre></div>
<pre><code>##            [,1]
## [1,]  4.0367180
## [2,] -0.2644476
## [3,]  1.1926823
## [4,]  1.9446203
## [5,]  0.0000000
## [6,] -1.0250517
## [7,]  0.0000000</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="analysis-of-covariance.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/15-GMTheory.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
